{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAONFevTsQ2t"
   },
   "source": [
    "# Aplicaciones a Problemas Directos\n",
    "\n",
    "**Por:** David Ortiz, Rodrigo Salas  \n",
    "**Edición:** David Ortiz, Tabita Catalán, Tomás Banduc\n",
    "\n",
    "### Introducción\n",
    "En esta lección extendemos el uso de PINNs desde EDOs hacia **ecuaciones diferenciales parciales (EDPs)**. Nos enfocaremos en un modelo de difusión lineal: la **ecuación de calor 1D**, con el objetivo de estudiar cómo una PINN puede aproximar soluciones espacio–temporales y cómo se construye la función de pérdida cuando la física está dada por una EDP.\n",
    "\n",
    "### Resumen de la actividad\n",
    "Se implementa una PINN para resolver la ecuación de calor en una dimensión espacial. El objetivo es consolidar el flujo de trabajo para EDPs lineales en un esquema **data-free** (sin datos observados), utilizando una solución analítica como referencia.\n",
    "\n",
    "### Objetivos de la actividad\n",
    "Al finalizar esta actividad, serás capaz de:\n",
    "- Formular un problema de EDP y construir una solución analítica de referencia.  \n",
    "- Entrenar una PINN para resolver una EDP lineal con un enfoque *data-free*.  \n",
    "\n",
    "### Descripción matemática del problema\n",
    "Consideramos el modelo de difusión unidimensional (ecuación de calor):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = \\kappa \\frac{\\partial^2 u}{\\partial x^2} + f(t,x),\n",
    "\\qquad x \\in [-1,1], \\quad t \\in [0,2], \\quad \\kappa \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "Aquí $u(t,x)$ representa una cantidad física (temperatura, concentración, voltaje, etc.), $\\kappa$ es el coeficiente de difusión y $f(t,x)$ es un término fuente.\n",
    "\n",
    "Para construir un caso con solución cerrada, fijamos $\\kappa = 1$ y proponemos\n",
    "\n",
    "$$\n",
    "u(t,x) = e^{-t}\\sin(\\pi x).\n",
    "$$\n",
    "\n",
    "Al sustituir en la EDP se obtiene el siguiente problema (con condiciones iniciales y de borde) cuya solución exacta es la función propuesta:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{EDP:}\\quad & \\frac{\\partial u}{\\partial t}\n",
    "= \\frac{\\partial^2 u}{\\partial x^2}\n",
    "- e^{-t}\\big(\\sin(\\pi x) - \\pi^2\\sin(\\pi x)\\big),\n",
    "&& x \\in [-1, 1],\\; t \\in [0, 2], \\\\\n",
    "\\text{CI:}\\quad & u(0,x) = \\sin(\\pi x),\n",
    "&& x \\in [-1, 1], \\\\\n",
    "\\text{CB:}\\quad & u(t,-1) = u(t,1) = 0,\n",
    "&& t \\in [0, 2], \\\\\n",
    "\\text{Solución:}\\quad & u(t,x) = e^{-t}\\sin(\\pi x). &&\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Flujo de trabajo\n",
    "1. Visualizar la solución analítica en una malla uniforme.  \n",
    "2. Muestrear el dominio espacio–temporal para entrenar la PINN.  \n",
    "3. Entrenar la PINN siguiendo las 6 etapas vistas, definiendo explícitamente la función de pérdida.  \n",
    "\n",
    "## Recordemos el esquema básico\n",
    "<img src=\"../figures/pinns_new_scheme.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "- (1) formular el modelo matemático y su solución analítica;  \n",
    "- (2) definir el dominio espacio–temporal;  \n",
    "- (3) implementar una ANN como aproximador de $u(t,x)$;  \n",
    "- (4) emplear diferenciación automática;  \n",
    "- (5) diseñar la *loss* con términos físicos (residuo de la EDP y condición inicial/de borde) usando `autograd`;  \n",
    "- (6) seleccionar un optimizador (Adam).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbbWY0_6sQ2u"
   },
   "source": [
    "### Configuración Inicial\n",
    "\n",
    "Comenzamos importando módulos y definiendo algunas funciones de utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHGJriHZsQ2u"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDti8oW6ALsY"
   },
   "outputs": [],
   "source": [
    "# NumPy para operaciones numéricas\n",
    "import numpy as np\n",
    "# PyTorch para construir y entrenar redes neuronales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Matplotlib para graficar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# Time para medir tiempo de entrenamiento\n",
    "import time\n",
    "# Warnings para ignorar mensajes de advertencia\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDsTqOmksQ2v"
   },
   "outputs": [],
   "source": [
    "# Setup (device + plots)\n",
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \\\n",
    "           \"mps\"  if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def set_mpl_style(gray: str = \"#5c5c5c\") -> None:\n",
    "    mpl.rcParams.update({\n",
    "        \"image.cmap\": \"viridis\",\n",
    "        \"text.color\": gray, \"xtick.color\": gray, \"ytick.color\": gray,\n",
    "        \"axes.labelcolor\": gray, \"axes.edgecolor\": gray,\n",
    "        \"axes.spines.right\": False, \"axes.spines.top\": False,\n",
    "        \"axes.formatter.use_mathtext\": True, \"axes.unicode_minus\": False,\n",
    "        \"font.size\": 15, \"interactive\": False, \"font.family\": \"sans-serif\",\n",
    "        \"legend.loc\": \"best\", \"text.usetex\": False, \"mathtext.fontset\": \"stix\",\n",
    "    })\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using {device} device\")\n",
    "set_mpl_style()\n",
    "\n",
    "# Definir pi en torch\n",
    "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "\n",
    "def relative_l2_error(u_num: torch.Tensor, u_ref: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.norm(u_num - u_ref) / torch.norm(u_ref)\n",
    "\n",
    "# Autodiff helper\n",
    "def grad(outputs: torch.Tensor, inputs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"d(outputs)/d(inputs) with create_graph=True.\"\"\"\n",
    "    return torch.autograd.grad(\n",
    "        outputs, inputs,\n",
    "        grad_outputs=torch.ones_like(outputs),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "def plot_comparison(u_true, u_pred, loss):\n",
    "    u_hat = u_pred.detach().cpu().numpy()\n",
    "\n",
    "    # --- Soluciones ---\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    for a, u, title in zip(\n",
    "        ax,\n",
    "        [u_true, u_hat],\n",
    "        ['Analytic solution for diffusion', 'PINN solution for diffusion']\n",
    "    ):\n",
    "        im = a.imshow(u, extent=[-1, 1, 2, 0])\n",
    "        a.set(title=title, xlabel=r'$x$', ylabel=r'$t$')\n",
    "        fig.colorbar(im, ax=a, shrink=0.5)\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --- Error + entrenamiento ---\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    im = ax[0].imshow(np.abs(u_true - u_hat), extent=[-1, 1, 2, 0])\n",
    "    ax[0].set(title=r'$|u(t,x) - u_{\\mathrm{pred}}(t,x)|$',\n",
    "              xlabel=r'$x$', ylabel=r'$t$')\n",
    "    fig.colorbar(im, ax=ax[0], shrink=0.5)\n",
    "\n",
    "    ax[1].plot(loss)\n",
    "    ax[1].set(title='Training Progress',\n",
    "              xlabel='Iteration', ylabel='Loss',\n",
    "              xscale='log', yscale='log')\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "    \n",
    "def animate(x,t,U):\n",
    "\n",
    "    # Primero, generar figura con subplot correspondiente\n",
    "    fig, ax = plt.subplots(figsize = (10,6))\n",
    "    plt.close()\n",
    "\n",
    "    ax.set_title(r\"heat solution $e^t sin(\\pi x)$\")\n",
    "\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    ax.set_ylim(np.floor(U.min()), np.ceil(U.max()))\n",
    "    ax.set_xlabel(r\"$x$\")\n",
    "    ax.set_ylabel(r\"$u(x, t)$\")\n",
    "\n",
    "    # Inicializar etiqueta sin texto\n",
    "    time_label = ax.text(1, 1, \"\", color = \"black\", fontsize = 12)\n",
    "    # Inicializar gráfico sin datos\n",
    "    line, = ax.plot([], [], color = \"black\", lw = 2)\n",
    "\n",
    "    # Definir función de inicialización\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        time_label.set_text(\"\")\n",
    "        return (line,)\n",
    "\n",
    "    # Animar función. Esta función se llama secuencialmente con FuncAnimation\n",
    "    def animate(i):\n",
    "        line.set_data(x, U[i])\n",
    "        time_label.set_text(f\"t = {t[i]:.2f}\")\n",
    "        return (line,)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                frames=len(t), interval=50, blit=True)\n",
    "\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualización de la solución Analítica\n",
    "Utilizamos la solución $u(t,x) = e^{-t}\\sin(\\pi x)$ para el problema de difusión y la evaluamos en coordenadas específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de muestras para espacio y tiempo.\n",
    "# Obs: Podrían tomarse distintos valores de muestreo en tiempo y en espacio, pero se considera un solo valor por simplicidad\n",
    "dom_samples = 100\n",
    "\n",
    "# TODO: Defina función para solución analítica\n",
    "# Indicación: utilice np.exp, np.sin y np.pi\n",
    "def analytic_diffusion(x,t):\n",
    "    u = ...\n",
    "    return u\n",
    "\n",
    "# Dominio espacial\n",
    "x_lower = -1\n",
    "x_upper = 1 \n",
    "x = np.linspace(x_lower, x_upper, dom_samples)\n",
    "# TODO: Definir límites del dominio temporal\n",
    "t_lower = ... \n",
    "t_upper = ...\n",
    "t = np.linspace(t_lower, t_upper, dom_samples)\n",
    "\n",
    "# Mallado\n",
    "X, T = np.meshgrid(x, t)\n",
    "# Evaluar función en mallado\n",
    "U = analytic_diffusion(X, T)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(X, T, U, cmap='viridis', edgecolor='k')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('t')\n",
    "ax.set_zlabel('u(t, x)')\n",
    "ax.set_title('3D Analytic Solution for Diffusion')\n",
    "\n",
    "# Añadir la barra de color\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correr animación y hacer display\n",
    "anim = animate(x,t,U)\n",
    "rc(\"animation\", html=\"jshtml\")\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQnX18mcsQ2x"
   },
   "source": [
    "## 2. Muestreo del dominio para el entrenamiento de la PINN\n",
    "\n",
    "Para entrenar la PINN se muestrea el dominio espacio–temporal utilizando **Muestreo de Hipercubo Latino** (*Latin Hypercube Sampling*, LHS). Esta estrategia estratificada permite cubrir de manera uniforme el espacio de entrada, reduciendo la formación de aglomeraciones al dividir el dominio en subregiones y seleccionar un punto aleatorio en cada una de ellas.\n",
    "\n",
    "En la implementación se utiliza `qmc.LatinHypercube` de `scipy.stats`, escalando posteriormente las muestras para que coincidan con los límites del dominio. Finalmente, los puntos muestreados se convierten a `torch.Tensor` para su uso directo en el entrenamiento de la PINN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POUvhO2SsQ2x"
   },
   "outputs": [],
   "source": [
    "# Muestreo con LHS\n",
    "def collocation_lhs(l_bounds, u_bounds, n=100, plot=True):\n",
    "    \"\"\"\n",
    "    n : int, opcional\n",
    "        Número de puntos de colocación a generar. Por defecto es 100.\n",
    "    l_bounds : tupla de float o int\n",
    "        Límites inferiores del dominio (x_lower, t_lower).\n",
    "    u_bounds : tupla de float o int\n",
    "        Límites superiores del dominio (x_upper, t_upper).\n",
    "    plot : bool, opcional (por defecto True)\n",
    "        Si es True, muestra un scatter plot de los puntos generados. \n",
    "    \"\"\"\n",
    "    sampler = qmc.LatinHypercube(d=2)\n",
    "    domain_xt = qmc.scale(sampler.random(n=n), l_bounds, u_bounds)\n",
    "\n",
    "    # Interior (PDE)\n",
    "    x_ten = torch.tensor(domain_xt[:, 0], requires_grad=True).float().view(-1, 1)\n",
    "    t_ten = torch.tensor(domain_xt[:, 1], requires_grad=True).float().view(-1, 1)\n",
    "\n",
    "    # Proyecciones desde los mismos puntos LHS\n",
    "    x_ic, t_ic = domain_xt[:, 0], np.full(n, l_bounds[1])      # IC: t = t_min\n",
    "    t_bc = domain_xt[:, 1]                                      # BC: usar los mismos t\n",
    "    x_bcL, x_bcR = np.full(n, l_bounds[0]), np.full(n, u_bounds[0])  # x = x_min / x_max\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.scatter(domain_xt[:, 0], domain_xt[:, 1], s=15, label='PDE (LHS)')\n",
    "        ax.scatter(x_ic, t_ic, s=25, label=f'IC: t={l_bounds[1]} ')\n",
    "        ax.scatter(x_bcL, t_bc, s=25, label=f'BC: x={l_bounds[0]} ')\n",
    "        ax.scatter(x_bcR, t_bc, s=25, label=f'BC: x={u_bounds[0]} ')\n",
    "        ax.set(title='Collocation points', xlabel=r'$x$', ylabel=r'$t$')\n",
    "        ax.legend(loc='best'); ax.invert_yaxis()\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    return x_ten, t_ten\n",
    "\n",
    "_ = collocation_lhs(l_bounds=(x_lower, t_lower), u_bounds=(x_upper, t_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXlfF7hMsQ2y"
   },
   "source": [
    "## Entrenamiento de la PINN\n",
    "\n",
    "En esta etapa se entrena una PINN para aproximar la solución espacio–temporal $u(t,x)$ del problema de difusión, es decir,\n",
    "$$\n",
    "u_{\\text{PINN}}(t,x;\\Theta) \\approx u(t,x),\n",
    "$$\n",
    "empleando la misma arquitectura y estrategia de entrenamiento utilizadas previamente.\n",
    "\n",
    "\n",
    "### Función de pérdida informada por la física (usando autodiff)\n",
    "\n",
    "Para entrenar la PINN se definen el residuo de la EDP, la condición inicial y las condiciones de borde. La solución exacta $u(t,x)$ se reemplaza por la salida de la red $u_{\\text{PINN}}(t,x;\\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\text{pde}}(t,x;u_{\\text{PINN}}) &=\n",
    "\\frac{\\partial u_{\\text{PINN}}}{\\partial t}\n",
    "- \\frac{\\partial^2 u_{\\text{PINN}}}{\\partial x^2}\n",
    "+ e^{-t}\\big(\\sin(\\pi x) - \\pi^2 \\sin(\\pi x)\\big), \\\\\n",
    "g_{\\text{ic}}(0,x;u_{\\text{PINN}}) &=\n",
    "u_{\\text{PINN}}(0,x;\\Theta) - \\sin(\\pi x), \\\\\n",
    "h_{\\text{bc1}}(t,-1;u_{\\text{PINN}}) &=\n",
    "u_{\\text{PINN}}(t,-1;\\Theta), \\\\\n",
    "h_{\\text{bc2}}(t,1;u_{\\text{PINN}}) &=\n",
    "u_{\\text{PINN}}(t,1;\\Theta).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "La función de pérdida informada por la física se construye utilizando el error cuadrático medio (MSE) como una combinación ponderada de los términos anteriores:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\Theta) =\\;&\n",
    "\\frac{\\lambda_1}{N}\\sum_i f_{\\text{pde}}(t_i,x_i;u_{\\text{PINN}})^2\n",
    "+ \\frac{\\lambda_2}{N}\\sum_i g_{\\text{ic}}(0,x_i;u_{\\text{PINN}})^2 \\\\\n",
    "&+ \\frac{\\lambda_3}{N}\\sum_i h_{\\text{bc1}}(t_i,-1;u_{\\text{PINN}})^2\n",
    "+ \\frac{\\lambda_3}{N}\\sum_i h_{\\text{bc2}}(t_i,1;u_{\\text{PINN}})^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "donde $\\lambda_{1,2,3} \\in \\mathbb{R}^+$ son coeficientes de ponderación y $N$ es el número de muestras.\n",
    "\n",
    "El entrenamiento de la PINN se formula como el siguiente problema de optimización:\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta} \\; \\mathcal{L}(\\Theta) \\rightarrow 0,\n",
    "$$\n",
    "\n",
    "utilizando diferenciación automática (`torch.autograd`) para evaluar las derivadas necesarias en el residuo físico del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9ZDoeyNsQ2y"
   },
   "outputs": [],
   "source": [
    "# Definir clase de red neuronal con capas y neuronas especificadas por usuario\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"Inicialización de parámetros Xavier Glorot\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presenta el código completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ETAPA 1: DEFINICIÓN DE LOS PARÁMETROS (MODELO FÍSICO)\n",
    "#===============================================================================\n",
    "# TODO: definir límites del dominio\n",
    "x_lower = ...\n",
    "x_upper = ...\n",
    "t_lower = ... \n",
    "t_upper = ...\n",
    "\n",
    "l_bounds = [x_lower, t_lower]\n",
    "u_bounds = [x_upper, t_upper]\n",
    "\n",
    "Ncoll_points = 100\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 2: DEFINICIÓN DEL DOMINIO \n",
    "#===============================================================================\n",
    "x_ten, t_ten = collocation_lhs(l_bounds, u_bounds, n=Ncoll_points, plot=False)\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 3: CREACIÓN DE LA RED NEURONAL SURROGANTE \n",
    "#===============================================================================\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# hiper-parámetros de la red\n",
    "hidden_layers = [2, 10, 10, 10, 1]\n",
    "\n",
    "# Crear instancia de la NN\n",
    "u_pinn = NeuralNetwork(hidden_layers)\n",
    "nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 4 Y 5: DEFINICIÓN DE LA FUNCIÓN DE COSTO BASADA EN AUTOGRAD\n",
    "#===============================================================================\n",
    "# Error cuadrático medio (Mean Squared Error - MSE)\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "def PINN_diffusion_Loss(forward_pass, x_ten, t_ten, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    domain = torch.cat([t_ten, x_ten], dim = 1)\n",
    "    u = forward_pass(domain)\n",
    "    u_t = ... # TODO: calculate derivative w/r to t, use t_ten\n",
    "    u_x = ... # TODO: calculate derivative w/r to x\n",
    "    u_xx = ... # TODO: calculate second derivative w/r to x \n",
    "    \n",
    "    # TODO: definir PDE loss \n",
    "    f_pde = ... # debería usar las variables t_ten, x_ten, u_t y u_xx, además de las funciones torch.sin, torch.exp\n",
    "    PDE_loss = lambda1 * MSE_func(f_pde, torch.zeros_like(f_pde)) \n",
    "    \n",
    "    # TODO: definir IC loss \n",
    "    ic = torch.cat([torch.zeros_like(t_ten), x_ten], dim = 1)\n",
    "    g_ic = ...\n",
    "    IC_loss = lambda2 * ...\n",
    "\n",
    "    # TODO: definir BC x = -1 loss\n",
    "    bc1 = torch.cat([t_ten, -torch.ones_like(x_ten)], dim = 1)\n",
    "    h_bc1 = ...\n",
    "    BC1_loss = lambda3 * ...\n",
    "    \n",
    "    # TODO: definir BC x = 1 loss\n",
    "    bc2 = torch.cat([t_ten, torch.ones_like(x_ten)], dim = 1)\n",
    "    h_bc2 = ...\n",
    "    BC2_loss = lambda3 * ...\n",
    "    \n",
    "    return PDE_loss + IC_loss + BC1_loss + BC2_loss\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 6: DEFINICIÓN DEl OPTIMIZADOR\n",
    "#===============================================================================\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(u_pinn.parameters(), lr=learning_rate,\n",
    "                        betas= (0.99,0.999), eps = 1e-8)\n",
    "\n",
    "\n",
    "#===============================================================================\n",
    "# CICLO DE ENTRENAMIENTO\n",
    "#===============================================================================\n",
    "training_iter = 15000\n",
    "\n",
    "# Inicializar lista para guardar valores de pérdida\n",
    "loss_values = []\n",
    "\n",
    "# Empezar timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenar red neuronal\n",
    "for i in range(training_iter):\n",
    "\n",
    "    optimizer.zero_grad()   # Reinicializar gradientes para iteración de entrenamiento\n",
    "\n",
    "    # ingresar x, predecir con PINN y obtener pérdida\n",
    "    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten)\n",
    "\n",
    "    # Agregar actual valor de pérdida a la lista\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    if i % 1000 == 0:  # Print cada 1000 iteraciones\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "\n",
    "    loss.backward() # Paso de retropropagación\n",
    "    optimizer.step() # Actualizar pesos de la red con optimizador\n",
    "\n",
    "# Detener timer y obtener tiempo transcurrido\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKUq3c71sQ2z"
   },
   "outputs": [],
   "source": [
    "# Graficación\n",
    "X_ten = torch.tensor(X).float().reshape(-1, 1)\n",
    "T_ten = torch.tensor(T).float().reshape(-1, 1)\n",
    "domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n",
    "U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n",
    "\n",
    "U_true = torch.tensor(U).float()\n",
    "print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n",
    "\n",
    "plot_comparison(U, U_pred, loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FYWSwocsQ2z"
   },
   "source": [
    "## **Ejercicios**:\n",
    "1. Agregar caso basado en datos. **Indicación**: Utilice los mismos puntos de colocación de la PINN.\n",
    "2. Evalúe cómo varía la solución de la PINN aumentando y disminuyendo los pesos `lambdas`.\n",
    "3. Evalúe cómo varía la solución de la PINN aumentando y disminuyendo la tasa de aprendizaje y el número de iteraciones de entrenamiento.\n",
    "4. Cambie el número de capas ocultas, neuronas y funciones de activación del modelo de NN.\n",
    "\n",
    "## **Questions**:\n",
    "1. ¿Por qué no es necesario incluir datos de la solución en el interior del dominio para resolver el problema de EDP con PINNs?\n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "    En el modelo de difusión, las leyes físicas son bien comprendidas y el problema asociado es bien puesto. El modelo de calor describe cómo una cantidad de interés cambia con el tiempo y cómo esta dinámica se relaciona con el espacio. Las propiedades de los operadores lineales asociados a la EDP de calor hacen que las soluciones al problema de difusión sean únicas y estables, siendo solamente necesarias las condiciones iniciales y de borde para conocerlas completamente al interior del dominio. Este comportamiento hace que no sea necesario tomar un enfoque basado en datos para conocer la solución del problema de difusión, haciendo que la PINN pueda prescindir de información empírica y solo tenga que minimizar residuos asociados a la formulación matemática de la EDP.\n",
    "   </details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
