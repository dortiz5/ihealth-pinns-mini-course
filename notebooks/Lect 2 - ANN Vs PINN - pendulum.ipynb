{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCgpgtSqTbvP"
   },
   "source": [
    "# Redes Neuronales Artificiales (ANN) vs. Redes Neuronales Informadas por la F√≠sica (PINNs)\n",
    "\n",
    "**Autores:** Tabita Catal√°n, Tom√°s Banduc, David Ortiz y Francisco Sahli ‚Äî 2025\n",
    "\n",
    "Accede al trabajo fundacional de las PINNs [aqu√≠](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
    "\n",
    "### Introducci√≥n\n",
    "\n",
    "En la lecci√≥n anterior se construy√≥ una PINN b√°sica siguiendo un esquema estructurado de seis pasos, aplicado a un sistema lineal cl√°sico (masa‚Äìresorte‚Äìamortiguador) con soluci√≥n anal√≠tica conocida. Ese ejercicio permiti√≥ establecer el marco conceptual y computacional de las PINNs, as√≠ como el rol de la f√≠sica en la funci√≥n de p√©rdida.\n",
    "\n",
    "En esta lecci√≥n se profundiza en ese marco mediante una comparaci√≥n directa entre **Redes Neuronales Artificiales (ANNs)** y **Redes Neuronales Informadas por la F√≠sica (PINNs)**. Mientras que las ANNs aprenden la soluci√≥n exclusivamente a partir de datos, las PINNs incorporan expl√≠citamente las ecuaciones gobernantes del sistema, lo que introduce restricciones f√≠sicas durante el entrenamiento.\n",
    "\n",
    "La comparaci√≥n se realiza sobre el modelo del **p√©ndulo oscilante**, un sistema no lineal que representa un incremento natural de complejidad respecto al caso lineal estudiado previamente.\n",
    "\n",
    "### Resumen de la actividad\n",
    "\n",
    "Se implementan dos aproximaciones para resolver el modelo matem√°tico no lineal de un p√©ndulo oscilante:\n",
    "\n",
    "- una ANN entrenada √∫nicamente a partir de datos,\n",
    "- y una PINN construida siguiendo el mismo esquema de seis pasos introducido en la lecci√≥n anterior.\n",
    "\n",
    "Este planteamiento permite aislar el efecto de la informaci√≥n f√≠sica en la funci√≥n de p√©rdida y comparar ambos enfoques en t√©rminos de desempe√±o, estabilidad y capacidad de generalizaci√≥n.\n",
    "\n",
    "### Objetivos de la actividad\n",
    "\n",
    "Al finalizar esta actividad, ser√°s capaz de:\n",
    "\n",
    "- Identificar las diferencias conceptuales y pr√°cticas entre ANNs y PINNs.  \n",
    "- Analizar el impacto de incorporar la f√≠sica del sistema en el proceso de entrenamiento.  \n",
    "- Implementar y entrenar ANNs y PINNs en PyTorch para un mismo problema no lineal.  \n",
    "- Evaluar comparativamente la calidad de las soluciones obtenidas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7OpUDIRTbvR"
   },
   "source": [
    "## Modelo matem√°tico para describir un p√©ndulo oscilante\n",
    "\n",
    "Queremos resolver el problema matem√°tico relacionado con el **p√©ndulo oscilante** [(wiki)](https://en.wikipedia.org/wiki/Pendulum_(mechanics)):\n",
    "\n",
    "| ![GIF](../figures/Oscillating_pendulum.gif?raw=1) | <img src=\"../figures/Pendulum_gravity.svg?raw=1\" alt=\"Diagrama del proyecto\" width=\"300\"/> |\n",
    "|-------------------------------------------|-------------------------------------------|\n",
    "| Vectores de velocidad y aceleraci√≥n del p√©ndulo  | Diagrama de fuerzas |\n",
    "\n",
    "\n",
    "**Supuestos:**\n",
    "\n",
    "- La varilla es r√≠gida y sin masa [(Tarea - el caso de una cuerda el√°stica)](https://en.wikipedia.org/wiki/Elastic_pendulum#:~:text=In%20physics%20and%20mathematics%2C%20in,%2Ddimensional%20spring%2Dmass%20system.).\n",
    "- El peso es una masa puntual.  \n",
    "- Dos dimensiones [(Tarea - una dimensi√≥n adicional de movimiento)](https://www.instagram.com/reel/CffUr64PjCx/?igsh=MWlmM2FscG9oYnp6bw%3D%3D).\n",
    "- No hay resistencia del aire [(Tarea - inmersi√≥n en un fluido)](https://www.youtube.com/watch?v=erveOJD_qv4&ab_channel=Lettherebemath).\n",
    "- El campo gravitacional es uniforme y el soporte no se mueve.\n",
    "\n",
    "Nos interesa encontrar el √°ngulo vertical $\\theta(t) \\in [0, 2\\pi)$ tal que:\n",
    "\n",
    "$$\n",
    "\\frac{d^2\\theta}{dt^2}+\\frac{g}{l}\\sin\\theta=0,\\quad\\theta(0)=\\theta_0,\\quad\\theta'(0)=0,\\quad t\\in\\mathbb{R},\n",
    "$$\n",
    "\n",
    "donde $g\\approx 9.81[m/s^2]$, $l$ es el largo de la varilla y $t$ la variable temporal.  \n",
    "\n",
    "**Repaso de conceptos de ecuaciones diferenciales:**\n",
    "\n",
    "- ¬øPor qu√© esta es una ecuaci√≥n diferencial no lineal? ¬øQu√© supuestos deber√≠an hacerse para linealizar el modelo?\n",
    "- ¬øEs una ecuaci√≥n diferencial ordinaria (EDO) o una ecuaci√≥n diferencial parcial (EDP)?  \n",
    "- ¬øCu√°l es el orden? ¬øcu√°l es el grado?  \n",
    "\n",
    "Un m√©todo √∫til es convertir el modelo en un sistema acoplado de EDOs:  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\theta}{dt} &= \\omega, \\quad \\text{(velocidad angular)}\\\\\n",
    "\\frac{d\\omega}{dt} & = -\\frac{g}{l}\\sin\\theta, \\quad \\text{(aceleraci√≥n angular)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Flujo de Trabajo  \n",
    "1. Calcular la soluci√≥n num√©rica del modelo no lineal del p√©ndulo oscilante y preparaci√≥n de los datos de entrenamiento a√±adiendo ruido, remuestreando y limitando el tiempo para simular un escenario real.  \n",
    "2. Definir el modelo ANN utilizando la arquitectura de PyTorch y entrenar con los datos preparados. Graficar la soluci√≥n.  \n",
    "3. Definir el modelo PINN utilizando la arquitectura de PyTorch y entrenar con los datos preparados. Graficar la soluci√≥n.  \n",
    "4. Comparar las soluciones obtenidas con ambas arquitecturas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gKS9hYYTbvS"
   },
   "source": [
    "## Configuraci√≥n Inicial  \n",
    "\n",
    "Comenzamos importando algunos paquetes √∫tiles y definiendo algunas funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvjIWMEtTbvS"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLwL3oIbTbvS",
    "outputId": "d732db5c-47d7-4528-8888-d66be1381707"
   },
   "outputs": [],
   "source": [
    "# Setup (device + plots)\n",
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \\\n",
    "           \"mps\"  if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def set_mpl_style(gray: str = \"#5c5c5c\") -> None:\n",
    "    mpl.rcParams.update({\n",
    "        \"image.cmap\": \"viridis\",\n",
    "        \"text.color\": gray, \"xtick.color\": gray, \"ytick.color\": gray,\n",
    "        \"axes.labelcolor\": gray, \"axes.edgecolor\": gray,\n",
    "        \"axes.spines.right\": False, \"axes.spines.top\": False,\n",
    "        \"axes.formatter.use_mathtext\": True, \"axes.unicode_minus\": False,\n",
    "        \"font.size\": 15, \"interactive\": False, \"font.family\": \"sans-serif\",\n",
    "        \"legend.loc\": \"best\", \"text.usetex\": False, \"mathtext.fontset\": \"stix\",\n",
    "    })\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using {device} device\")\n",
    "set_mpl_style()\n",
    "\n",
    "# Metrics\n",
    "def add_noise(signal, snr_db):\n",
    "    noise_power = np.mean(signal**2) / (10**(snr_db / 10))\n",
    "    noise = np.sqrt(noise_power) * np.random.randn(*signal.shape)\n",
    "    return signal + noise, noise\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal, noise = np.asarray(signal), np.asarray(noise)\n",
    "    return 10 * np.log10(np.mean(signal**2) / np.mean(noise**2))\n",
    "\n",
    "def relative_l2_error(u_num: torch.Tensor, u_ref: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.norm(u_num - u_ref) / torch.norm(u_ref)\n",
    "\n",
    "# Autodiff helper\n",
    "def grad(outputs: torch.Tensor, inputs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"d(outputs)/d(inputs) with create_graph=True.\"\"\"\n",
    "    return torch.autograd.grad(\n",
    "        outputs, inputs,\n",
    "        grad_outputs=torch.ones_like(outputs),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "# Plotting\n",
    "def plot_comparison(t: torch.Tensor, theta_true, theta_pred: torch.Tensor, loss) -> None:\n",
    "    t_np = t.detach().cpu().numpy().ravel()\n",
    "    pred_np = theta_pred.detach().cpu().numpy().ravel()\n",
    "    true_np = np.asarray(theta_true).ravel()\n",
    "    diff = np.abs(true_np - pred_np)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].plot(t_np, true_np, label=r'$\\theta(t)$ (numerical)')\n",
    "    axs[0].plot(t_np, pred_np, label=r'$\\theta_{\\mathrm{pred}}(t)$')\n",
    "    axs[0].set(title='Numerical vs Predicted', xlabel=r'Time $(s)$', \n",
    "               ylabel='Amplitude', ylim=(-1, 1.3))\n",
    "    axs[0].legend(frameon=False)\n",
    "\n",
    "    axs[1].plot(t_np, diff)\n",
    "    axs[1].set(title='Absolute Difference', xlabel=r'Time $(s)$', \n",
    "               ylabel=r'$|\\theta - \\theta_{\\mathrm{pred}}|$')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    ax.plot(loss)\n",
    "    ax.set(title='Training Progress', xlabel='Iteration', \n",
    "           ylabel='Loss', xscale='log', yscale='log')\n",
    "    ax.grid(True)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtSeOyujTbvT"
   },
   "source": [
    "## 1. Soluci√≥n num√©rica del p√©ndulo oscilante y preparaci√≥n de los datos\n",
    "\n",
    "El objetivo de esta etapa es generar datos sint√©ticos del p√©ndulo oscilante que se utilizar√°n como referencia durante el entrenamiento, bajo los supuestos establecidos previamente.\n",
    "\n",
    "### 1.1 Soluci√≥n num√©rica\n",
    "\n",
    "La soluci√≥n num√©rica del modelo se obtiene mediante el m√©todo de [Runge-Kutta de cuarto orden](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods), implementado en `scipy`. Para ello se definen los par√°metros del sistema, el modelo matem√°tico del p√©ndulo y el dominio temporal de simulaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77rbOuBRTbvT"
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Par√°metros del sistema\n",
    "g, L = 9.81, 1.0 # Gravedad (m/s^2) y longitud de la varilla (m) \n",
    "theta0, omega0 = np.pi / 4, 0.0 # condiciones iniciales, √°ngulo (rad) y velocidad angular (rad/s) \n",
    "fs, T = 100, 10 # Frecuencia de muestreo (Hz) y tiempo total (s) \n",
    "\n",
    "t_eval = np.linspace(0, T, fs * T)\n",
    "y0 = [theta0, omega0]\n",
    "\n",
    "# definimos el sistema de ecuaciones diferenciales\n",
    "def pendulum(t, y):\n",
    "    theta, omega = y\n",
    "    dtheta_dt = omega\n",
    "    domega_dt = -(g / L) * np.sin(theta)\n",
    "    return [dtheta_dt, domega_dt]\n",
    "\n",
    "# Soluci√≥n num√©rica\n",
    "num_sol = solve_ivp(pendulum, (0, T), y0, t_eval=t_eval, method=\"RK45\")\n",
    "theta_num, omega_num = num_sol.y\n",
    "\n",
    "# Gr√°fica\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(t_eval, theta_num, label=r'$\\theta(t)$ [rad]')\n",
    "plt.plot(t_eval, omega_num, label=r'$\\omega(t)$ [rad/s]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylim(-2.5, 3.3)\n",
    "plt.title('Nonlinear Pendulum Solution')\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaZRsQHvTbvU"
   },
   "source": [
    "### 1.2 Preparaci√≥n de los datos de entrenamiento <a id=\"data_prep\"></a>\n",
    "\n",
    "La soluci√≥n num√©rica se utiliza como **datos de entrenamiento**, interpretados como mediciones provenientes de un sensor. Para simular un escenario experimental realista, se a√±ade ruido gaussiano, se remuestrean los datos y se recorta la se√±al a un intervalo de $2.5\\,\\text{s}$.\n",
    "\n",
    "Como medida cuantitativa del nivel de distorsi√≥n introducido, se calcula la relaci√≥n se√±al‚Äìruido\n",
    "\n",
    "$$\n",
    "\\mathrm{SNR} = 10 \\log_{10}\\!\\left(\\frac{P_{\\text{signal}}}{P_{\\text{noise}}}\\right),\n",
    "$$\n",
    "\n",
    "donde $P_{\\text{signal}}$ y $P_{\\text{noise}}$ corresponden a la potencia de la se√±al y del ruido, respectivamente. Notar que si $P_{\\text{signal}}=P_{\\text{noise}}$, $SNR = 0dB$, y si $P_{\\text{signal}}>>P_{\\text{noise}}$, $SNR<0dB$. \n",
    "\n",
    "Para este ejercicio consideramos $SNR = 10dB$ y la funci√≥n `add_noise` para agregar el ruido Gaussiano.\n",
    "\n",
    "En lo que sigue, los datos de entrenamiento ruidosos se denotan por $\\theta_{\\text{data}}(t)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "agIihTIyTbvV",
    "outputId": "fd129006-9880-4b5c-cb8d-7ec2df27ca63"
   },
   "outputs": [],
   "source": [
    "# Add gaussian noise\n",
    "theta_noisy, noise = add_noise(theta_num, 10)  \n",
    "print(f'SNR: {calculate_snr(theta_noisy, noise):.4f} dB')\n",
    "\n",
    "t_max = 2.5           # seconds\n",
    "step = 5              # downsampling factor\n",
    "idx = slice(0, int(t_max * fs), step)\n",
    "\n",
    "theta_data = theta_noisy[idx]\n",
    "t_data = t_eval[idx]\n",
    "\n",
    "# We graph the observed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta_num, label=r'Angular Displacement (model) $\\theta(t)$ ')\n",
    "plt.plot(t_data, theta_data, label=r'Training data (measures) $\\theta_{data}(t)$ ')\n",
    "plt.xlabel(r'Time $[s]$')\n",
    "plt.ylabel(r'Angular displacement $[rad]$')\n",
    "plt.ylim(-1,1.3)\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.title('Training data')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PpIuan2TbvV"
   },
   "source": [
    "## 2. Entrenando la Red Neuronal Artificial\n",
    "\n",
    "Entrenaremos la red neuronal artificial para aproximar directamente la soluci√≥n de la ecuaci√≥n diferencial, es decir,\n",
    "\n",
    "$$\n",
    "\\theta_{NN}(t; \\Theta) \\approx \\theta(t)\n",
    "$$\n",
    "\n",
    "donde $\\Theta$ son los par√°metros entrenables de la ANN. Utilizaremos `PyTorch` para definir la red y la entrenaremos con el optimizador ADAM. Adem√°s, convertiremos el dominio temporal y las observaciones a `torch.tensors`. \n",
    "\n",
    "### Funci√≥n de p√©rdida \n",
    "\n",
    "Para entrenar la ANN necesitamos datos y una funci√≥n de p√©rdida. Nuestros datos ser√°n observaciones ruidosas de la soluci√≥n $\\theta_{data}(t)$, obtenidas en puntos de colocaci√≥n $\\{t_i\\}_N$ elegidos del dominio. Utilizamos como funci√≥n de p√©rdida el error cuadr√°tico medio ($MSE$) entre estas observaciones y la evaluaci√≥n de la ANN en los mismos puntos de colocaci√≥n, es decir,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) := \\lambda_1 MSE(\\theta_{NN}(t; \\Theta), \\theta_{data}(t)) = \\frac{\\lambda_1}{N}\\sum_i (\\theta_{NN}(t_i; \\Theta) - \\theta_{data}(t_i))^2\n",
    "$$\n",
    "\n",
    "donde $\\lambda_1 \\in \\mathbb{R}^+$ es un peso positivo y $N$ es el n√∫mero de muestras. El entrenamiento se realiza minimizando la funci√≥n de p√©rdida $\\mathcal{L}(\\Theta)$, es decir,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta \\in \\mathbb{R}} \\mathcal{L}(\\Theta) \\rightarrow 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n se define la clase de la red neuronal que se utilizar√° tanto en la ANN como en la PINN.\n",
    "\n",
    "> **üí° Nota**  \n",
    "> Para efectos comparativos de esta lecci√≥n, los hiperpar√°metros de la red neuronal se mantendr√°n fijos: n√∫mero de capas ocultas, n√∫mero de neuronas por capa, funci√≥n de activaci√≥n `tanh`, tasa de aprendizaje y n√∫mero de √©pocas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "#%% Hyperpar√°metros para el entrenamiento\n",
    "torch.manual_seed(123)\n",
    "hidden_layers = [1, 50, 50, 50, 1]  # Hiperpar√°metros de la red ()\n",
    "learning_rate = 0.001               # \n",
    "training_iter = 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el c√≥digo completo para el entrenamiento de la ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ETAPA 1: INFORMACI√ìN DEL MODELO F√çSICO\n",
    "#===============================================================================\n",
    "# Numerical theta to test Numpy array to pytorch tensor\n",
    "theta_test = torch.tensor(theta_num, device=device, requires_grad=True).view(-1,1).float()\n",
    "# Numerical theta to train Numpy array to pytorch tensor\n",
    "theta_data = torch.tensor(theta_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 2: DEFINICI√ìN DEL DOMINIO \n",
    "#===============================================================================\n",
    "# Convert the NumPy arrays to PyTorch tensors and add an extra dimension\n",
    "# test time Numpy array to Pytorch tensor\n",
    "t_test = torch.tensor(t_eval, device=device, requires_grad=True).view(-1,1).float()\n",
    "# train time Numpy array to Pytorch tensor\n",
    "t_data = torch.tensor(t_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 3: CREACI√ìN DE LA RED NEURONAL SURROGANTE \n",
    "#===============================================================================\n",
    "# Create an instance of the neural network\n",
    "theta_ann = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in theta_ann.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 5: DEFINICI√ìN DE LA FUNCI√ìN DE COSTO BASADA √öNICAMENTE EN LOS DATOS\n",
    "#==========================================================================\n",
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "def NeuralNetworkLoss(forward_pass, t, theta_data, lambda1 = 1):\n",
    "\n",
    "    theta_nn = forward_pass(t)\n",
    "    data_loss = lambda1 * MSE_func(theta_nn, theta_data)\n",
    "\n",
    "    return  data_loss\n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 6: DEFINICI√ìN DEl OPTIMIZADOR\n",
    "#==========================================================================\n",
    "optimizer = optim.Adam(theta_ann.parameters(), lr=learning_rate,\n",
    "                         betas= (0.99,0.999), eps = 1e-8)\n",
    "\n",
    "#==========================================================================\n",
    "# CICLO DE ENTRENAMIENTO\n",
    "#==========================================================================\n",
    "# Initialize a list to store the loss values\n",
    "loss_values_ann = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = NeuralNetworkLoss(theta_ann,\n",
    "                             t_data,\n",
    "                             theta_data)    # must be (1. nn output, 2. target)\n",
    "\n",
    "    # Append the current loss value to the list\n",
    "    loss_values_ann.append(loss.item())\n",
    "\n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "\n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sITbWTKXTbvW"
   },
   "source": [
    "graficamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "id": "53BRPDdjTbvX",
    "outputId": "febce2b5-b3bf-418d-8cf5-d9129a792ede"
   },
   "outputs": [],
   "source": [
    "theta_pred_ann = theta_ann(t_test).to(device)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred_ann, theta_test)}')\n",
    "\n",
    "plot_comparison(t_test, theta_num, theta_pred_ann, loss_values_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wS25mOJTbvX"
   },
   "source": [
    "## 3. Entrenamiento de la Red Neuronal Informada por la F√≠sica (PINN)\n",
    "\n",
    "En esta etapa se entrena una PINN para aproximar la soluci√≥n de la ecuaci√≥n diferencial ordinaria del p√©ndulo,\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{PINN}}(t;\\Theta) \\approx \\theta(t).\n",
    "$$\n",
    "\n",
    "La arquitectura de la red es la misma utilizada previamente para la ANN. La diferencia radica en el proceso de entrenamiento: adem√°s de las observaciones ruidosas, se incorporan expl√≠citamente las ecuaciones f√≠sicas que gobiernan la din√°mica del sistema.\n",
    "\n",
    "### Funci√≥n de p√©rdida informada por la f√≠sica\n",
    "\n",
    "Recordemos el modelo del p√©ndulo y definamos el residuo de la ecuaci√≥n diferencial, junto con la condici√≥n inicial sobre el desplazamiento y la velocidad angular. La soluci√≥n anal√≠tica $\\theta(t)$ se reemplaza por la salida de la red $\\theta_{\\text{PINN}}(t;\\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\text{ode}}(t;\\theta_{\\text{PINN}}) &=\n",
    "\\frac{d^2 \\theta_{\\text{PINN}}(t;\\Theta)}{dt^2}\n",
    "+ \\frac{g}{l}\\sin\\!\\big(\\theta_{\\text{PINN}}(t;\\Theta)\\big), \\\\\n",
    "g_{\\text{ic}}(0;\\theta_{\\text{PINN}}) &=\n",
    "\\theta_{\\text{PINN}}(0;\\Theta) - \\theta_0, \\\\\n",
    "h_{\\text{bc}}(0;\\theta_{\\text{PINN}}) &=\n",
    "\\frac{d\\theta_{\\text{PINN}}(0;\\Theta)}{dt}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "La funci√≥n de p√©rdida informada por la f√≠sica se construye utilizando el error cuadr√°tico medio (MSE) como una combinaci√≥n ponderada de t√©rminos f√≠sicos y de datos:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\Theta) =\\;&\n",
    "\\frac{\\lambda_1}{N}\\sum_i f_{\\text{ode}}(t_i;\\theta_{\\text{PINN}})^2 \\\\\n",
    "+ & \\lambda_2\\, g_{\\text{ic}}(0;\\theta_{\\text{PINN}})^2 \\\\\n",
    "+ & \\lambda_3\\, h_{\\text{bc}}(0;\\theta_{\\text{PINN}})^2 \\\\\n",
    "+ &\\frac{\\lambda_4}{N}\\sum_i\n",
    "\\big(\\theta_{\\text{PINN}}(t_i;\\Theta) - \\theta_{\\text{data}}(t_i)\\big)^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "donde $\\lambda_{1,2,3,4} \\in \\mathbb{R}^+$ son coeficientes de ponderaci√≥n y $N$ es el n√∫mero de muestras.\n",
    "\n",
    "> **üí° Nota**  \n",
    "> Si la funci√≥n de p√©rdida incluye √∫nicamente t√©rminos f√≠sicos, el esquema es *data-free*.  \n",
    "> Al incorporar t√©rminos asociados a datos observados, el enfoque pasa a ser *data-driven*.\n",
    "\n",
    "El entrenamiento de la PINN se realiza resolviendo el problema de optimizaci√≥n\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta} \\; \\mathcal{L}(\\Theta) \\rightarrow 0.\n",
    "$$\n",
    "\n",
    "> **üí° Nota**  \n",
    "> La diferenciaci√≥n autom√°tica (`torch.autograd`) permite calcular las derivadas de la salida de la red respecto a la entrada temporal, lo que resulta esencial para evaluar los t√©rminos f√≠sicos de la funci√≥n de p√©rdida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ETAPA 1: DEFINICI√ìN DE LOS PAR√ÅMETROS (MODELO F√çSICO)\n",
    "#===============================================================================\n",
    "# Par√°metros del sistema\n",
    "g, L = 9.81, 1.0 # Gravedad (m/s^2) y longitud de la varilla (m) \n",
    "theta0, omega0 = np.pi / 4, 0.0 # condiciones iniciales, √°ngulo (rad) y velocidad angular (rad/s) \n",
    "\n",
    "# Numerical theta to test Numpy array to pytorch tensor\n",
    "theta_test = torch.tensor(theta_num, device=device, requires_grad=True).view(-1,1).float()\n",
    "# Numerical theta to train Numpy array to pytorch tensor\n",
    "theta_data = torch.tensor(theta_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 2: DEFINICI√ìN DEL DOMINIO \n",
    "#===============================================================================\n",
    "# Convert the NumPy arrays to PyTorch tensors and add an extra dimension\n",
    "# test time Numpy array to Pytorch tensor\n",
    "t_test = torch.tensor(t_eval, device=device, requires_grad=True).view(-1,1).float()\n",
    "# train time Numpy array to Pytorch tensor\n",
    "t_data = torch.tensor(t_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 3: CREACI√ìN DE LA RED NEURONAL SURROGANTE \n",
    "#===============================================================================\n",
    "# Create an instance of the neural network\n",
    "theta_pinn = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in theta_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 4 Y 5: DEFINICI√ìN DE LA FUNCI√ìN DE COSTO BASADA EN LA F√çSICA\n",
    "#===============================================================================\n",
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# derivatives of the ANN\n",
    "def PINNLoss(forward_pass, t_phys, t_data, theta_data, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1, lambda4 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    theta_pinn1 = forward_pass(t_phys)\n",
    "    theta_pinn_dt = grad(theta_pinn1, t_phys)\n",
    "    theta_pinn_ddt = grad(theta_pinn_dt, t_phys)\n",
    "    \n",
    "    f_ode = theta_pinn_ddt + (g/L) * torch.sin(theta_pinn1)\n",
    "    ODE_loss = lambda1 * MSE_func(f_ode, torch.zeros_like(f_ode)) \n",
    "    \n",
    "    # Define t = 0 for boundary an initial conditions\n",
    "    t0 = torch.tensor(0., device=device, requires_grad=True).view(-1,1)\n",
    "    \n",
    "    g_ic = forward_pass(t0)\n",
    "    IC_loss = lambda2 * MSE_func(g_ic, torch.ones_like(g_ic)*theta0)\n",
    "    \n",
    "    h_bc = grad(forward_pass(t0),t0)\n",
    "    BC_loss = lambda3 * MSE_func(h_bc, torch.zeros_like(h_bc))\n",
    "    \n",
    "    theta_nn2 = forward_pass(t_data)\n",
    "    data_loss = lambda4 * MSE_func(theta_nn2, theta_data)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss + data_loss\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 6: DEFINICI√ìN DEl OPTIMIZADOR\n",
    "#===============================================================================\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(theta_pinn.parameters(), lr=learning_rate,\n",
    "                       betas= (0.99,0.999), eps = 1e-8)\n",
    "\n",
    "#===============================================================================\n",
    "# CICLO DE ENTRENAMIENTO\n",
    "#===============================================================================\n",
    "# Initialize a list to store the loss values\n",
    "loss_values_pinn = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINNLoss(theta_pinn, t_test, t_data, theta_data)\n",
    "\n",
    "    # Append the current loss value to the list\n",
    "    loss_values_pinn.append(loss.item())\n",
    "\n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "\n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G70N-xiTbvY"
   },
   "source": [
    "Nuevamente, graficamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "id": "sLEu3EJbTbvY",
    "outputId": "391ca19f-ebdf-4b56-ea72-ab9d4512646d"
   },
   "outputs": [],
   "source": [
    "theta_pred_pinn = theta_pinn(t_test)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred_pinn, theta_test)}')\n",
    "\n",
    "plot_comparison(t_test, theta_num, theta_pred_pinn, loss_values_pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3DZxHeBTbvY"
   },
   "source": [
    "## 4. Comparaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z1e1kIGaTbvZ",
    "outputId": "ac388224-0634-4895-b5f4-1aba257b94bf"
   },
   "outputs": [],
   "source": [
    "plot_comparison(t_test, theta_num, theta_pred_ann, loss_values_ann)\n",
    "plot_comparison(t_test, theta_num, theta_pred_pinn, loss_values_pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfSHJ_0hTbvZ"
   },
   "source": [
    "## **Ejercicios**:\n",
    "\n",
    "1. Elimina la p√©rdida de los datos del entrenamiento de la PINN. ¬øA√∫n se puede obtener la soluci√≥n?\n",
    "2. Incrementa y reduce el par√°metro `std_deviation` en la secci√≥n [Preparaci√≥n de los datos de entrenamiento](#data_prep) para cambiar el `SNR`. Tambi√©n cambia las variables `resample` y `ctime`, y compara los resultados tras entrenar la ANN y la PINN.\n",
    "3. Ajusta los valores de los par√°metros `lambdas` en la funci√≥n de p√©rdida para ambas redes y analiza su impacto.\n",
    "4. Modifica la tasa de aprendizaje (`learning_rate`) del optimizador y el n√∫mero de iteraciones de entrenamiento, y eval√∫a el efecto en el desempe√±o.\n",
    "5. Cambia el n√∫mero de capas ocultas (`hidden_layers`), neuronas y funciones de activaci√≥n de la red neuronal, y observa el impacto en los resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **¬øEn qu√© formas son ventajosas las PINNs comparadas con los m√©todos num√©ricos tradicionales, considerando el mayor tiempo requerido para el entrenamiento?**  \n",
    "   <details>\n",
    "   <summary>Respuesta</summary>\n",
    "   Las PINNs ofrecen varias ventajas frente a los m√©todos num√©ricos tradicionales, a pesar de su tiempo de entrenamiento generalmente m√°s largo. Una ventaja clave es su flexibilidad para manejar dominios complejos, de alta dimensionalidad y geometr√≠as irregulares sin requerir mallas estructuradas. Las PINNs tambi√©n pueden incorporar f√°cilmente restricciones o datos adicionales, como mediciones experimentales o condiciones de frontera. A diferencia de muchos m√©todos num√©ricos, una vez entrenadas, las PINNs generalizan bien a diferentes condiciones iniciales y de frontera, lo que puede hacerlas m√°s vers√°tiles en escenarios que requieren simulaciones repetidas o ajustes de par√°metros. Esta flexibilidad y adaptabilidad las convierten en una herramienta poderosa para ciertas tareas de modelado informadas por f√≠sica donde los m√©todos tradicionales pueden estar limitados.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pinns-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
