{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MC6DyCDmZUq"
      },
      "source": [
        "# Temas avanzados sobre las PINNs\n",
        "\n",
        "**Autor:** David Ortiz ‚Äî 2025\n",
        "\n",
        "Accede al trabajo fundacional de las PINNs [aqu√≠](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
        "\n",
        "Este tutorial se apoya en la literatura reciente sobre *spectral bias* en redes neuronales y su impacto en el entrenamiento de PINNs.\n",
        "\n",
        "### Introducci√≥n\n",
        "Las Redes Neuronales Informadas por la F√≠sica (PINNs) heredan de las redes neuronales profundas un fen√≥meno conocido como **sesgo espectral**, mediante el cual las componentes de baja frecuencia de la soluci√≥n se aprenden m√°s r√°pidamente que las de alta frecuencia. Este comportamiento puede afectar de manera significativa la convergencia y precisi√≥n de las PINNs, especialmente en problemas con soluciones multiescalares o gradientes pronunciados. En esta actividad se analiza y caracteriza el sesgo espectral en PINNs, utilizando un problema controlado con soluci√≥n conocida para estudiar su efecto en el entrenamiento.\n",
        "\n",
        "### Objetivos de aprendizaje\n",
        "- Comprender el concepto de sesgo espectral en redes neuronales y su manifestaci√≥n en PINNs  \n",
        "- Analizar el impacto del sesgo espectral en la convergencia de soluciones f√≠sicas  \n",
        "- Evaluar estrategias simples para mitigar el sesgo espectral en PINNs  \n",
        "\n",
        "### Resumen de la Actividad\n",
        "Estudiar el sesgo espectral en PINNs a partir de un problema unidimensional con soluci√≥n anal√≠tica conocida, siguiendo el mismo esquema general de implementaci√≥n:\n",
        "\n",
        "<img src=\"../figures/pinns_new_scheme.png\" width=\"800\" height=\"400\">\n",
        "\n",
        "- (1) formular el problema f√≠sico y su soluci√≥n exacta;  \n",
        "- (2) definir el dominio de entrenamiento;  \n",
        "- (3) implementar una ANN como aproximador de la soluci√≥n;  \n",
        "- (4) analizar el contenido espectral de la soluci√≥n y de la red;  \n",
        "- (5) entrenar la PINN y estudiar la evoluci√≥n espectral del error;  \n",
        "- (6) comparar resultados bajo distintas configuraciones de entrenamiento.\n",
        "\n",
        "Finalmente, discutir c√≥mo el sesgo espectral influye en la capacidad de las PINNs para capturar din√°micas de alta frecuencia.\n",
        "\n",
        "\n",
        "## Descripci√≥n Matem√°tica del Problema\n",
        "\n",
        "### **Contexto**\n",
        "\n",
        "El sistema de Lorenz es un conjunto de tres ecuaciones diferenciales ordinarias acopladas que describe un sistema ca√≥tico. Fue introducido por Edward Lorenz en 1963 como un modelo simplificado de convecci√≥n atmosf√©rica y se ha convertido en uno de los ejemplos m√°s ic√≥nicos de comportamiento ca√≥tico en sistemas din√°micos.\n",
        "\n",
        "### **Modelo Matem√°tico**\n",
        "\n",
        "El sistema de Lorenz est√° definido por las siguientes ecuaciones:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dx}{dt} &= \\sigma(y - x) \\\\\n",
        "\\frac{dy}{dt} &= x(\\rho - z) - y \\\\\n",
        "\\frac{dz}{dt} &= xy - \\beta z\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $x(t)$, $y(t)$, $z(t)$ son las variables de estado del sistema\n",
        "- $\\sigma$ (sigma) es el n√∫mero de Prandtl, que representa la relaci√≥n entre viscosidad y conductividad t√©rmica\n",
        "- $\\rho$ (rho) es el n√∫mero de Rayleigh, relacionado con la diferencia de temperatura\n",
        "- $\\beta$ (beta) es un par√°metro geom√©trico relacionado con las dimensiones f√≠sicas del sistema\n",
        "\n",
        "### **Par√°metros y Condiciones Iniciales**\n",
        "\n",
        "Para este ejercicio, utiliza los **par√°metros cl√°sicos de Lorenz** que producen comportamiento ca√≥tico:\n",
        "\n",
        "$$\n",
        "\\sigma = 10.0, \\quad \\rho = 28.0, \\quad \\beta = \\frac{8}{3}\n",
        "$$\n",
        "\n",
        "**Condiciones iniciales:**\n",
        "\n",
        "$$\n",
        "x(0) = 1.0, \\quad y(0) = 1.0, \\quad z(0) = 1.0\n",
        "$$\n",
        "\n",
        "**Dominio temporal:** $t \\in [0, 20]$\n",
        "\n",
        "### **Objetivo**\n",
        "\n",
        "Implementar una **Physics-Informed Neural Network (PINN)** siguiendo las 6 etapas del flujo de trabajo para:\n",
        "\n",
        "1. Aproximar las soluciones $x(t)$, $y(t)$, $z(t)$ del sistema de Lorenz\n",
        "2. Comparar los resultados con la soluci√≥n num√©rica obtenida mediante el m√©todo de Runge-Kutta\n",
        "3. Visualizar el famoso **atractor de Lorenz** en el espacio de fases $(x, y, z)$\n",
        "4. Analizar la capacidad de la PINN para capturar la din√°mica ca√≥tica del sistema\n",
        "\n",
        "> **üí° REMARK:**  \n",
        "> Para este ejercicio debes utilizar un enfoque data-driven. Es decir, en la \n",
        "> funci√≥n de costo debes incluir los datos simulados. Si tienes dudas de como \n",
        "> se hace, puedes ver la soluci√≥n del ejercicio en la carpeta \"solutions\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx9qvQjdmZUr"
      },
      "source": [
        "### Configuraci√≥n Inicial\n",
        "\n",
        "Comenzamos importando m√≥dulos y definiendo algunas funciones de utilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gufADy3jmZUr"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A3G-YUetrKY4"
      },
      "outputs": [],
      "source": [
        "# NumPy para operaciones num√©ricas\n",
        "import numpy as np\n",
        "# PyTorch para construir y entrenar redes neuronales\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Matplotlib para graficar\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mlp\n",
        "# Time para medir tiempo de entrenamiento\n",
        "import time\n",
        "# Warnings para ignorar mensajes de advertencia\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9zNpH05mZUs"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Actualizaci√≥n de los par√°metros de Matplotlib\n",
        "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
        "mlp.rcParams.update(\n",
        "    {\n",
        "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
        "        \"text.color\" : gray,\n",
        "        \"xtick.color\" :gray,\n",
        "        \"ytick.color\" :gray,\n",
        "        \"axes.labelcolor\" : gray,\n",
        "        \"axes.edgecolor\" :gray,\n",
        "        \"axes.spines.right\" : False,\n",
        "        \"axes.spines.top\" : False,\n",
        "        \"axes.formatter.use_mathtext\": True,\n",
        "        \"axes.unicode_minus\": False,\n",
        "\n",
        "        'font.size' : 15,\n",
        "        'interactive': False,\n",
        "        \"font.family\": 'sans-serif',\n",
        "        \"legend.loc\" : 'best',\n",
        "        'text.usetex': False,\n",
        "        'mathtext.fontset': 'stix',\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# Util function to calculate the relative l2 error\n",
        "def relative_l2_error(u_num, u_ref):\n",
        "    # Calculate the L2 norm of the difference\n",
        "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
        "\n",
        "    # Calculate the L2 norm of the reference\n",
        "    l2_ref = torch.norm(u_ref, p=2)\n",
        "\n",
        "    # Calculate L2 relative error\n",
        "    relative_l2 = l2_diff / l2_ref\n",
        "    return relative_l2\n",
        "\n",
        "# Util function to plot the solutions\n",
        "def plot_comparison(time, theta_true, theta_pred, loss):\n",
        "\n",
        "    # Convert tensors to numpy arrays for plotting\n",
        "    t_np = time.detach().cpu().data.numpy()\n",
        "    theta_true_np = theta_true.detach().cpu().data.numpy()\n",
        "    theta_pred_np = theta_pred.detach().cpu().data.numpy()\n",
        "\n",
        "\n",
        "    # Create a figure with 2 subplots\n",
        "    _, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Plot the true and predicted values\n",
        "    axs[0].plot(t_np, theta_true_np, label = r'$\\theta(t)$ (numerical solution)')\n",
        "    axs[0].plot(t_np, theta_pred_np, label = r'$\\theta_{pred}(t)$ (predicted solution) ')\n",
        "    axs[0].set_title('Angular displacement Numerical Vs. Predicted')\n",
        "    axs[0].set_xlabel(r'Time $(s)$')\n",
        "    axs[0].set_ylabel('Amplitude')\n",
        "    axs[0].set_ylim(-1,1.3)\n",
        "    axs[0].legend(loc='best', frameon=False)\n",
        "\n",
        "\n",
        "    # Plot the difference between the predicted and true values\n",
        "    difference = np.abs(theta_true_np.reshape(-1,1) - theta_pred_np.reshape(-1,1))\n",
        "    axs[1].plot(t_np, difference)\n",
        "    axs[1].set_title('Absolute Difference')\n",
        "    axs[1].set_xlabel(r'Time $(s)$')\n",
        "    axs[1].set_ylabel(r'$|\\theta(t) - \\theta_{pred}(t)|$')\n",
        "    # Display the plot\n",
        "    plt.legend(loc='best', frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the loss values recorded during training\n",
        "    # Create a figure with 1 subplots\n",
        "    _, axs = plt.subplots(1, 1, figsize=(6, 3))\n",
        "    axs.plot(loss)\n",
        "    axs.set_xlabel('Iteration')\n",
        "    axs.set_ylabel('Loss')\n",
        "    axs.set_yscale('log')\n",
        "    axs.set_xscale('log')\n",
        "    axs.set_title('Training Progress')\n",
        "    axs.grid(True)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.integrate import solve_ivp\n",
        "\n",
        "#===============================================================================\n",
        "# ARQUITECTURA MODIFICADA: NO TOCAR\n",
        "#===============================================================================\n",
        "\n",
        "class Sin(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, hlayers, fourier_dim=None, sigma=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hlayers (list): lista con el n√∫mero de neuronas en cada capa.\n",
        "            fourier_dim (int): dimensi√≥n de las Fourier features (opcional).\n",
        "            sigma (float): escala de las frecuencias aleatorias.\n",
        "        \"\"\"\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.fourier_dim = fourier_dim\n",
        "        self.sigma = sigma\n",
        "\n",
        "        if self.fourier_dim is not None:\n",
        "            # Inicializamos matriz B ~ N(0, sigma^2)\n",
        "            input_dim = hlayers[0]\n",
        "            B = torch.randn((fourier_dim, input_dim)) * sigma\n",
        "            self.register_buffer(\"B\", B)  # se guarda como parte del modelo pero no se entrena\n",
        "            # actualizamos la entrada de la red: ahora 2*fourier_dim\n",
        "            hlayers = [2 * fourier_dim] + hlayers[1:]\n",
        "\n",
        "        layers = []\n",
        "        for i in range(len(hlayers[:-2])):\n",
        "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
        "            layers.append(Sin())\n",
        "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.init_params()\n",
        "\n",
        "    def fourier_features(self, x):\n",
        "        \"\"\"Mapeo de Fourier features\"\"\"\n",
        "        # x shape: [batch, input_dim]\n",
        "        x_proj = 2 * torch.pi * x @ self.B.T  # [batch, fourier_dim]\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\"\"\"\n",
        "        def init_normal(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "        self.apply(init_normal)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.fourier_dim is not None:\n",
        "            x = self.fourier_features(x)\n",
        "        return self.layers(x)\n",
        "    \n",
        "#===============================================================================\n",
        "# SOLUCI√ìN NUM√âRICA. NO TOCAR\n",
        "#===============================================================================\n",
        "\n",
        "def numerical_sol_lorenz(t_eval, sigma = 10, rho = 28, beta = 8.0/3.0,\n",
        "                         x0 = 1, y0 = 1, z0 = 1):\n",
        "        \n",
        "    def lorenz_system(t, state, sigma, rho, beta):\n",
        "        x, y, z = state\n",
        "        dx_dt = sigma * (y - x)\n",
        "        dy_dt = x * (rho - z) - y\n",
        "        dz_dt = x * y - beta * z\n",
        "        return [dx_dt, dy_dt, dz_dt]\n",
        "\n",
        "    # Resolver con m√©todo num√©rico\n",
        "    t_span = (0, T)\n",
        "    t_eval_np = t_eval.detach().cpu().numpy().ravel()\n",
        "    initial_state = [x0, y0, z0]\n",
        "\n",
        "    sol = solve_ivp(\n",
        "        lorenz_system, \n",
        "        t_span, \n",
        "        initial_state,\n",
        "        args=(sigma, rho, beta),\n",
        "        t_eval=t_eval_np,\n",
        "        method='RK45'\n",
        "    )\n",
        "\n",
        "    x_true = torch.tensor(sol.y[0],device=device, dtype=torch.float32).view(-1, 1)\n",
        "    y_true = torch.tensor(sol.y[1],device=device, dtype=torch.float32).view(-1, 1)\n",
        "    z_true = torch.tensor(sol.y[2],device=device, dtype=torch.float32).view(-1, 1)\n",
        "    \n",
        "    return x_true, y_true, z_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#===============================================================================\n",
        "# ETAPA 1: DEFINICI√ìN DE LOS PAR√ÅMETROS (MODELO F√çSICO)\n",
        "#===============================================================================\n",
        "\n",
        "sigma = 10.0\n",
        "rho = 28.0\n",
        "beta = 8.0/3.0\n",
        "\n",
        "x0 = 1.0\n",
        "y0 = 1.0\n",
        "z0 = 1.0\n",
        "\n",
        "# Dominio temporal\n",
        "T = 20.0        # tiempo total de simulaci√≥n\n",
        "N_train = 101   # puntos de colocaci√≥n para entrenamiento\n",
        "N_eval = 2000   # puntos para evaluaci√≥n\n",
        "\n",
        "#===============================================================================\n",
        "# ETAPA 2: DEFINICI√ìN DEL DOMINIO \n",
        "#===============================================================================\n",
        "t_train, t_eval = crear_dominio_temporal(T, N_train, N_eval)\n",
        "\n",
        "x_train, y_train, z_train = numerical_sol_lorenz(t_train)\n",
        "\n",
        "#===============================================================================\n",
        "# ETAPA 3: CREACI√ìN DE LA RED NEURONAL SURROGANTE \n",
        "#===============================================================================\n",
        "\n",
        "# Crear la red\n",
        "torch.manual_seed(123)\n",
        "hidden_layers = [1, 50, 50, 50, 3]\n",
        "lorenz_pinn = NeuralNetwork(hidden_layers).to(device)\n",
        "nparams = sum(p.numel() for p in lorenz_pinn.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {nparams}')\n",
        "\n",
        "#==========================================================================\n",
        "# ETAPA 4 Y 5: DEFINICI√ìN DE LA FUNCI√ìN DE COSTO BASADA EN AUTOGRAD\n",
        "#==========================================================================\n",
        "MSE = nn.MSELoss()\n",
        "\n",
        "def LorenzPINNLoss(PINN, t_phys, sigma, rho, beta, \n",
        "                   x0=1.0, y0=1.0, z0=1.0,\n",
        "                   lambda_pde=10.0, lambda_ic=10.0):\n",
        "    \n",
        "    t0 = torch.tensor(0., device=device, requires_grad=True).view(-1, 1)\n",
        "    \n",
        "    # Predicciones de la red\n",
        "    u = PINN(t_phys)  # (N, 3)\n",
        "    x, y, z = u[:, 0:1], u[:, 1:2], u[:, 2:3]\n",
        "    \n",
        "    # Calcular derivadas temporales\n",
        "    dx_dt = grad(x, t_phys)\n",
        "    dy_dt = grad(y, t_phys)\n",
        "    dz_dt = grad(z, t_phys)\n",
        "    \n",
        "    # Residuos de las ecuaciones de Lorenz\n",
        "    f1 = dx_dt - sigma * (y - x)\n",
        "    f2 = dy_dt - (x * (rho - z) - y)\n",
        "    f3 = dz_dt - (x * y - beta * z)\n",
        "    \n",
        "    # P√©rdida PDE (residuo de las ecuaciones)\n",
        "    loss_pde = MSE(f1, torch.zeros_like(f1)) + \\\n",
        "               MSE(f2, torch.zeros_like(f2)) + \\\n",
        "               MSE(f3, torch.zeros_like(f3))\n",
        "    \n",
        "    # Condiciones iniciales\n",
        "    u0 = PINN(t0)\n",
        "    x0_pred, y0_pred, z0_pred = u0[:, 0:1], u0[:, 1:2], u0[:, 2:3]\n",
        "    \n",
        "    loss_ic = MSE(x0_pred, torch.ones_like(x0_pred) * x0) + \\\n",
        "              MSE(y0_pred, torch.ones_like(y0_pred) * y0) + \\\n",
        "              MSE(z0_pred, torch.ones_like(z0_pred) * z0)\n",
        "              \n",
        "    loss_data = MSE(x, x_train) + \\\n",
        "                MSE(y, y_train) + \\\n",
        "                MSE(z, z_train)\n",
        "    \n",
        "    # P√©rdida total\n",
        "    return lambda_pde * loss_pde + lambda_ic * loss_ic + 100*loss_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#==========================================================================\n",
        "# ETAPA 6: DEFINICI√ìN DEl OPTIMIZADOR\n",
        "#==========================================================================\n",
        "lr = 0.01\n",
        "optimizer = pinn_optimizer(lorenz_pinn, lr)\n",
        "\n",
        "#==========================================================================\n",
        "# CICLO DE ENTRENAMIENTO\n",
        "#==========================================================================\n",
        "training_iter = 30000\n",
        "loss_values = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(training_iter):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss = LorenzPINNLoss(lorenz_pinn, t_train, sigma, rho, beta)\n",
        "    \n",
        "    loss_values.append(loss.item())\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "        print(f\"Iteration {i}: Loss {loss.item():.10f}\")\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Training time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "#==========================================================================\n",
        "# Validaci√≥n\n",
        "#==========================================================================\n",
        "\n",
        "# Primero, obt√©n la soluci√≥n num√©rica completa para graficar\n",
        "x_true, y_true, z_true = numerical_sol_lorenz(t_eval, sigma, rho, beta, x0, y0, z0)\n",
        "\n",
        "# Convertir a numpy para graficar\n",
        "x_true_np = x_true.detach().cpu().numpy().ravel()\n",
        "y_true_np = y_true.detach().cpu().numpy().ravel()\n",
        "z_true_np = z_true.detach().cpu().numpy().ravel()\n",
        "\n",
        "# Predicciones de la PINN\n",
        "lorenz_pinn.eval()\n",
        "with torch.no_grad():\n",
        "    u_pred = lorenz_pinn(t_eval).cpu().numpy()\n",
        "    x_pred = u_pred[:, 0]\n",
        "    y_pred = u_pred[:, 1]\n",
        "    z_pred = u_pred[:, 2]\n",
        "\n",
        "t_plot = t_eval.detach().cpu().numpy().ravel()\n",
        "\n",
        "# Gr√°fico 1: Series temporales\n",
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
        "\n",
        "axes[0].plot(t_plot, x_true_np, 'g-', label='Soluci√≥n num√©rica', linewidth=2)\n",
        "axes[0].plot(t_plot, x_pred, 'r--', label='PINN', linewidth=1.5)\n",
        "axes[0].set_ylabel('x(t)', fontsize=12)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(t_plot, y_true_np, 'g-', label='Soluci√≥n num√©rica', linewidth=2)\n",
        "axes[1].plot(t_plot, y_pred, 'r--', label='PINN', linewidth=1.5)\n",
        "axes[1].set_ylabel('y(t)', fontsize=12)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(t_plot, z_true_np, 'g-', label='Soluci√≥n num√©rica', linewidth=2)\n",
        "axes[2].plot(t_plot, z_pred, 'r--', label='PINN', linewidth=1.5)\n",
        "axes[2].set_xlabel('Tiempo (s)', fontsize=12)\n",
        "axes[2].set_ylabel('z(t)', fontsize=12)\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gr√°fico 2: Atractor de Lorenz (3D)\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Soluci√≥n num√©rica\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax1.plot(x_true_np, y_true_np, z_true_np, 'g-', linewidth=0.5, alpha=0.7)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_zlabel('z')\n",
        "ax1.set_title('Atractor de Lorenz - Soluci√≥n Num√©rica')\n",
        "ax1.view_init(elev=20, azim=45)  # Ajustar vista\n",
        "\n",
        "# PINN\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "ax2.plot(x_pred, y_pred, z_pred, 'r-', linewidth=0.5, alpha=0.7)\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.set_zlabel('z')\n",
        "ax2.set_title('Atractor de Lorenz - PINN')\n",
        "ax2.view_init(elev=20, azim=45)  # Ajustar vista\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gr√°fico 3: P√©rdida durante el entrenamiento\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_values)\n",
        "plt.xlabel('Iteraci√≥n')\n",
        "plt.ylabel('P√©rdida')\n",
        "plt.yscale('log')\n",
        "plt.title('Convergencia del Entrenamiento')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Gr√°fico 4: Errores relativos\n",
        "\n",
        "# Convertir predicciones a tensores de PyTorch si no lo son\n",
        "x_pred_torch = torch.tensor(x_pred, device=device, dtype=torch.float32).view(-1, 1)\n",
        "y_pred_torch = torch.tensor(y_pred, device=device, dtype=torch.float32).view(-1, 1)\n",
        "z_pred_torch = torch.tensor(z_pred, device=device, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Calcular errores (ahora ambos son tensores de PyTorch)\n",
        "error_x = torch.norm(x_pred_torch - x_true) / torch.norm(x_true)\n",
        "error_y = torch.norm(y_pred_torch - y_true) / torch.norm(y_true)\n",
        "error_z = torch.norm(z_pred_torch - z_true) / torch.norm(z_true)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"M√âTRICAS DE ERROR\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Error L2 relativo en x: {error_x.item():.6f}\")\n",
        "print(f\"Error L2 relativo en y: {error_y.item():.6f}\")\n",
        "print(f\"Error L2 relativo en z: {error_z.item():.6f}\")\n",
        "print(f\"Error L2 promedio: {((error_x + error_y + error_z)/3).item():.6f}\")\n",
        "print(f\"{'='*50}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
