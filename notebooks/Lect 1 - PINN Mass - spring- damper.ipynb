{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCgpgtSqTbvP"
   },
   "source": [
    "# PINNs para el sistema masa‚Äìresorte‚Äìamortiguador\n",
    "\n",
    "**Autor:** David Ortiz ‚Äî 2025\n",
    "\n",
    "Accede al trabajo fundacional de las PINNs [aqu√≠](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
    "\n",
    "Este tutorial est√° inspirado en [este](https://github.com/benmoseley/harmonic-oscillator-pinn-workshop) workshop, elaborado por Ben Moseley\n",
    "\n",
    "### Introducci√≥n\n",
    "Las Redes Neuronales Informadas por la F√≠sica (PINNs) incorporan ecuaciones gobernantes en la funci√≥n de p√©rdida mediante diferenciaci√≥n autom√°tica, reduciendo la dependencia de grandes vol√∫menes de datos y mejorando la interpretabilidad. En esta actividad aplicaremos PINNs al modelo lineal cl√°sico **masa‚Äìresorte‚Äìamortiguador**, para el cual existe soluci√≥n anal√≠tica, lo que permitir√° verificar cuantitativamente el desempe√±o del enfoque.\n",
    "\n",
    "### Objetivos de de aprendizaje\n",
    "- Implementar una PINN desde cero en PyTorch para resolver el sistema masa-resorte-amortiguador siguiendo las 6 etapas vistas\n",
    "- Comprender los diferentes componentes de la funci√≥n de p√©rdida de una PINN (residuo de la EDO, condiciones iniciales y de frontera\n",
    "- Entender en mayor detalle c√≥mo se entrenan las PINNs y c√≥mo usar diferenciaci√≥n autom√°tica para calcular derivadas de redes neuronales\n",
    "\n",
    "### Resumen de la Actividad\n",
    "Construir una PINN para el sistema masa-resorte-amortiguador, basandonos en los 6 pasos vistos en el taller te√≥rico:\n",
    "\n",
    "<img src=\"../figures/pinns_new_scheme.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "- (1) formular el modelo matem√°tico y su soluci√≥n anal√≠tica; \n",
    "- (2) definir el dominio (temporal); \n",
    "- (3) implementar una ANN como surrogate de $x(t)$;\n",
    "- (4) Diferenciaci√≥n autom√°tica\n",
    "- (5) dise√±ar la loss con t√©rminos f√≠sicos (residuo de la EDO e inicio/condici√≥n inicial) usando autograd; \n",
    "- (6) fijar un optimizador (Adam);\n",
    "\n",
    "Finalmete, ejecutar el ciclo de entrenamiento y prueba, comparando contra la soluci√≥n exacta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importamos funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLwL3oIbTbvS",
    "outputId": "d732db5c-47d7-4528-8888-d66be1381707"
   },
   "outputs": [],
   "source": [
    "# Setup (device + plots)\n",
    "def get_device() -> str:\n",
    "    return \"cuda\" if torch.cuda.is_available() else \\\n",
    "           \"mps\"  if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def set_mpl_style(gray: str = \"#5c5c5c\") -> None:\n",
    "    mpl.rcParams.update({\n",
    "        \"image.cmap\": \"viridis\",\n",
    "        \"text.color\": gray, \"xtick.color\": gray, \"ytick.color\": gray,\n",
    "        \"axes.labelcolor\": gray, \"axes.edgecolor\": gray,\n",
    "        \"axes.spines.right\": False, \"axes.spines.top\": False,\n",
    "        \"axes.formatter.use_mathtext\": True, \"axes.unicode_minus\": False,\n",
    "        \"font.size\": 15, \"interactive\": False, \"font.family\": \"sans-serif\",\n",
    "        \"legend.loc\": \"best\", \"text.usetex\": False, \"mathtext.fontset\": \"stix\",\n",
    "    })\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using {device} device\")\n",
    "set_mpl_style()\n",
    "\n",
    "# Metrics\n",
    "def relative_l2_error(u_num: torch.Tensor, u_ref: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.norm(u_num - u_ref) / torch.norm(u_ref)\n",
    "\n",
    "# Util function to plot the solutions\n",
    "def plot_comparison(t, theta_true, theta_pred, loss):\n",
    "    t, u, u_hat = (\n",
    "        x.detach().cpu().numpy().ravel()\n",
    "        for x in (t, theta_true, theta_pred)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax[0].plot(t, u, label=r'$\\theta(t)$ (numerical)')\n",
    "    ax[0].plot(t, u_hat, label=r'$\\theta_{\\mathrm{pred}}(t)$')\n",
    "    ax[0].set(title='Numerical vs Predicted',\n",
    "              xlabel=r'Time $(s)$', ylabel='Amplitude', ylim=(-1, 1.3))\n",
    "    ax[0].legend(frameon=False)\n",
    "\n",
    "    ax[1].plot(t, np.abs(u - u_hat))\n",
    "    ax[1].set(title='Absolute Difference',\n",
    "              xlabel=r'Time $(s)$',\n",
    "              ylabel=r'$|\\theta - \\theta_{\\mathrm{pred}}|$')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.plot(loss)\n",
    "    ax.set(title='Training Progress',\n",
    "           xlabel='Iteration', ylabel='Loss',\n",
    "           xscale='log', yscale='log')\n",
    "    ax.grid(True)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modelo matem√°tico del sistema masa‚Äìresorte‚Äìamortiguador\n",
    "\n",
    "El sistema masa‚Äìresorte‚Äìamortiguador describe el movimiento de una masa $m$ sujeta a un resorte de constante el√°stica $k$ y un elemento disipador o amortiguador con coeficiente $c$. Su ecuaci√≥n de movimiento, en ausencia de fuerzas externas, est√° dada por:\n",
    "\n",
    "\\begin{equation*}\n",
    "m\\,\\ddot{x}(t) + c\\,\\dot{x}(t) + k\\,x(t) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "donde:\n",
    "- $x(t)$ es el desplazamiento de la masa respecto a su posici√≥n de equilibrio,  \n",
    "- $\\dot{x}(t)$ es la velocidad,  \n",
    "- $\\ddot{x}(t)$ es la aceleraci√≥n.\n",
    "\n",
    "Dividiendo entre $m$, se obtiene la forma normalizada:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ddot{x}(t) + 2\\zeta\\omega_n\\,\\dot{x}(t) + \\omega_n^2\\,x(t) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "con:\n",
    "\\begin{equation*}\n",
    "\\omega_n = \\sqrt{\\frac{k}{m}} \\quad \\text{(frecuencia natural no amortiguada)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\zeta = \\frac{c}{2\\sqrt{km}} \\quad \\text{(raz√≥n de amortiguamiento adimensional)}\n",
    "\\end{equation*}\n",
    "\n",
    "Definimos los siguientes par√°metros con los que vamos a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dominio temporal\n",
    "T = 5.0        # tiempo total de simulaci√≥n\n",
    "x0 = 1.0       # Posici√≥n inicial \n",
    "v0 = 0.0       # velocidad incial\n",
    "wn = 5.0       # Frecuencia natural\n",
    "zeta = 0.2     # raz√≥n de amortiguamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Definici√≥n del dominio (temporal) \n",
    "\n",
    "En esta etapa se define el dominio sobre el cual la PINN ser√° entrenada. El tiempo $t$ se discretiza en un conjunto de puntos uniformemente espaciados dentro del intervalo $[0, T]$. Estos puntos se usar√°n como entradas para la red neuronal durante el entrenamiento, mientras que una malla m√°s densa se emplear√° para evaluar la soluci√≥n exacta y comparar el desempe√±o de la PINN.  \n",
    "\n",
    "> **üí° REMARK!:**  \n",
    "> Cada punto $t_i \\in [0,T],\\ i = 1, \\dots, N_{sample}$ se conoce como **punto de colocaci√≥n**, \n",
    "> y su nombre proviene de la similitud de las PINNs con los m√©todos de colocaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dominio_temporal(T, N_train=101, N_eval=1000):\n",
    "    \"\"\"Crea el dominio temporal para la PINN.\"\"\"\n",
    "    t_train = torch.linspace(0, T, N_train, \n",
    "                             device=device, \n",
    "                             requires_grad=True).view(-1, 1)  # entrenamiento\n",
    "    t_eval = torch.linspace(0, T, N_eval, \n",
    "                             device=device, \n",
    "                             requires_grad=True).view(-1, 1)  # evaluaci√≥n\n",
    "    return t_train, t_eval # dominio de evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Red neuronal como aproximador de la soluci√≥n\n",
    "\n",
    "En esta etapa se define una red neuronal que aproxima la soluci√≥n de la ecuaci√≥n diferencial mediante un modelo param√©trico. La funci√≥n desconocida $x(t)$ se reemplaza por una red neuronal dependiente de un conjunto de par√°metros $\\Theta$:\n",
    "\n",
    "$$\n",
    "x_{PINN}(t; \\Theta) \\approx x_{real}(t).\n",
    "$$\n",
    "\n",
    "Los par√°metros de la red est√°n dados por los pesos y sesgos de cada capa, agrupados como:\n",
    "\n",
    "$$\n",
    "\\Theta = \\{ W_i, b_i \\}_{i=1}^{L},\n",
    "$$\n",
    "\n",
    "donde $W_i$ y $b_i$ representan respectivamente los pesos y sesgos de la capa $i$, y $L$ es el n√∫mero total de capas de la red. Estos par√°metros ser√°n ajustados durante el entrenamiento para minimizar el error entre la ecuaci√≥n f√≠sica y la salida de la red.\n",
    "\n",
    "> **üí° REMARK 1:**  \n",
    "> Para este ejercicio utilizaremos un perceptr√≥n multicapa, con funciones de \n",
    "> activaci√≥n tahn, y una inicializaci√≥n de pesos con esquema de Glorot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hlayers = [1, 10, 10, 1]):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Diferenciaci√≥n autom√°tica en PyTorch \n",
    "\n",
    "Antes de continuar ser√° √∫til aprender a calcular las derivadas de una red neuronal. Para esto utilizaremos la diferenciaci√≥n autom√°tica (*autodiff*), que es una t√©cnica para calcular gradientes de funciones de forma eficiente y precisa mediante el uso de la regla de la cadena. PyTorch registra la forma en que operamos las variables en un gr√°fico computacional din√°mico, que luego le permite calcular derivadas autom√°ticamente al realizar una \"propagaci√≥n hacia atr√°s\" (*backpropagation*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to calculate tensor gradients with autodiff\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect\n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                        grad_outputs=torch.ones_like(outputs),\n",
    "                        create_graph=True,\n",
    "                        )[0]\n",
    "\n",
    "\n",
    "# Definir tensor de entrada. Si queremos derivar c/r a x necesitamos inicializar con requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device, requires_grad=True).view(-1,1).float() # (N,1)\n",
    "\n",
    "# Calcular operaci√≥n que dependen de x\n",
    "y = x**2 # (N,1)  \n",
    "\n",
    "# Calcular derivadas c/r a x \n",
    "# grad es un wrapper de torch.autograd\n",
    "dy_dx = grad(y, x) \n",
    "\n",
    "# Calcular derivadas de orden superior\n",
    "d2y_dx2 = grad(dy_dx, x)  \n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y = x^2:\", y)\n",
    "print(\"dy/dx:\", dy_dx)\n",
    "print(\"d^2y/dx^2:\", d2y_dx2)  \n",
    "\n",
    "# Esto tambi√©n funciona para redes neuronales\n",
    "# test_ANN = NeuralNetwork()\n",
    "\n",
    "# NNx = test_ANN(x)\n",
    "# dNNx_dx = grad(NNx, x)\n",
    "# print(\"NNx: \", dNNx_dx)\n",
    "# print(\"dNNx/dx:\", dNNx_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Funci√≥n de p√©rdida informada por la f√≠sica del problema\n",
    "\n",
    "Para entrenar la PINN, utilizamos el modelo del sistema masa‚Äìresorte‚Äìamortiguador en su forma normalizada, donde la frecuencia natural $\\omega_n$ y la raz√≥n de amortiguamiento $\\zeta$ caracterizan la din√°mica:\n",
    "\n",
    "$$\n",
    "\\ddot{x}(t) + 2\\zeta\\omega_n\\,\\dot{x}(t) + \\omega_n^2 x(t) = 0\n",
    "$$\n",
    "\n",
    "Reemplazamos la soluci√≥n desconocida por la salida de la red neuronal $x_{PINN}(t;\\Theta)$ y definimos el residuo f√≠sico y las condiciones iniciales:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{ode}(t;\\,x_{PINN}) := &\\;\n",
    "\\frac{d^2x_{PINN}(t; \\Theta)}{dt^2}\n",
    "+ 2\\zeta\\omega_n \\frac{dx_{PINN}(t; \\Theta)}{dt}\n",
    "+ \\omega_n^2 x_{PINN}(t; \\Theta) = 0, \\\\\n",
    "g_{ic}(0;\\,x_{PINN}) :=&\\;\n",
    "x_{PINN}(0;\\Theta) - x_0 = 0, \\\\\n",
    "h_{ic}(0;\\,x_{PINN}) :=&\\;\n",
    "\\frac{dx_{PINN}(0;\\Theta)}{dt} - v_0 = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "La funci√≥n de p√©rdida total que se optimiza es:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\Theta) := &\n",
    "\\frac{\\lambda_1}{N}\\sum_i \\left( f_{ode}(t_i;\\,x_{PINN} - 0) \\right)^2\\\\\n",
    "+ &\\lambda_2 \\left( g_{ic}(0;\\,x_{PINN}) - x_0 \\right)^2\\\\\n",
    "+ &\\lambda_3 \\left( h_{ic}(0;\\,x_{PINN}) - v_0 \\right)^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "El entrenamiento busca minimizar esta funci√≥n:\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta} \\mathcal{L}(\\Theta) \\rightarrow 0\n",
    "$$\n",
    "\n",
    "utilizando diferenciaci√≥n autom√°tica (`torch.autograd`) para calcular las derivadas de la red con respecto al tiempo, necesarias para evaluar el residuo f√≠sico del modelo.\n",
    "\n",
    "> **üí° REMARK:**  \n",
    "> Cuando no incluimos la funci√≥n de p√©rdida relacionada con los datos, estamos \n",
    "> empleando un esquema independiente de datos (*data-free*); cuando incluimos una funci√≥n de p√©rdida relacionada a los datos, estamos empleando un esquema basado en datos (*data-driven*).\n",
    "\n",
    "\n",
    "> **üí° REMARK:**  \n",
    "> En este esquema, las condiciones iniciales y de frontera se cumplen d√©bilmente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# derivatives of the ANN\n",
    "def PINNLoss(PINN, t_phys, wn, zeta, x0 = 1, v0 = 0,\n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1):\n",
    "\n",
    "    t0 = torch.tensor(0., device=device, requires_grad=True).view(-1,1)\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    x_pinn_t = PINN(t_phys)\n",
    "    x_pinn_dt = grad(x_pinn_t, t_phys)\n",
    "    x_pinn_ddt = grad(x_pinn_dt, t_phys)\n",
    "    \n",
    "    f_ode = x_pinn_ddt + 2 * zeta * wn * x_pinn_dt + wn**2 * x_pinn_t\n",
    "    ODE_loss = lambda1 * MSE_func(f_ode, torch.zeros_like(f_ode)) \n",
    "    \n",
    "    g_ic = PINN(t0)\n",
    "    IC_loss = lambda2 * MSE_func(g_ic, torch.ones_like(g_ic)*x0)\n",
    "    \n",
    "    h_bc = grad(PINN(t0),t0)\n",
    "    BC_loss = lambda3 * MSE_func(h_bc, torch.zeros_like(h_bc)*v0)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Definici√≥n del optimizador\n",
    "\n",
    "Una vez definida la funci√≥n de p√©rdida, se selecciona un optimizador para ajustar los par√°metros $\\Theta = \\{W_i, b_i\\}$ de la red neuronal. El objetivo del optimizador es minimizar la funci√≥n de p√©rdida informada por la f√≠sica, es decir:\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta} \\mathcal{L}(\\Theta) \\quad \\text{via} \\quad \\Theta^{k+1}=\\Theta^{k}-\\alpha\\nabla_\\Theta\\mathcal{L}(\\Theta^{k})\n",
    "$$\n",
    "\n",
    "En esta etapa se emplea el optimizador **ADAM**, un m√©todo de descenso de gradiente adaptativo que ajusta din√°micamente la tasa de aprendizaje para cada par√°metro. ADAM combina las ventajas del momento cl√°sico y del escalado adaptativo de gradientes, lo que lo hace especialmente adecuado para el entrenamiento de PINNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_optimizer(pinn, lr = 0.01):\n",
    "\n",
    "    # Define an optimizer (Adam) for training the network\n",
    "    return optim.Adam(pinn.parameters(), lr=lr,\n",
    "                        betas= (0.99,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento\n",
    "\n",
    "En esta etapa se ejecuta el proceso iterativo mediante el cual la PINN ajusta sus par√°metros $\\Theta = \\{W_i, b_i\\}$ para minimizar la funci√≥n de p√©rdida definida anteriormente. Durante cada √©poca (*epoch*), el modelo eval√∫a el residuo de la ecuaci√≥n diferencial y las condiciones iniciales sobre los puntos de colocaci√≥n $t_i \\in [0, T]$, actualizando los par√°metros seg√∫n el gradiente de la p√©rdida:\n",
    "\n",
    "$$\n",
    "\\Theta \\leftarrow \\Theta - \\eta \\, \\nabla_\\Theta \\mathcal{L}(\\Theta)\n",
    "$$\n",
    "\n",
    "donde $\\eta$ es la tasa de aprendizaje. El ciclo contin√∫a hasta que la p√©rdida alcanza un valor suficientemente peque√±o o deja de mejorar significativamente. Finalmente, la soluci√≥n obtenida $x_{PINN}(t; \\Theta)$ se compara con la soluci√≥n exacta para evaluar la calidad del entrenamiento y la capacidad del modelo de reproducir la din√°mica del sistema masa‚Äìresorte‚Äìamortiguador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n se presenta el c√≥digo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# ETAPA 1: DEFINICI√ìN DE LOS PAR√ÅMETROS (MODELO F√çSICO)\n",
    "#===============================================================================\n",
    "# Dominio temporal\n",
    "T = 5.0        # tiempo total de simulaci√≥n\n",
    "x0 = 1.0       # Posici√≥n inicial \n",
    "v0 = 0.0       # velocidad incial\n",
    "wn = 5.0  # Frecuencia natural\n",
    "zeta = 0.2     # raz√≥n de amortiguamiento\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 2: DEFINICI√ìN DEL DOMINIO \n",
    "#===============================================================================\n",
    "# Creamos los tensores de tiempo para el entrenamiento y la evaluaci√≥n\n",
    "t_train, t_eval = crear_dominio_temporal(T)\n",
    "\n",
    "#===============================================================================\n",
    "# ETAPA 3: CREACI√ìN DE LA RED NEURONAL SURROGANTE \n",
    "#===============================================================================\n",
    "# Creamos la ANN\n",
    "torch.manual_seed(123)\n",
    "hidden_layers = [1, 30, 30, 30, 1]# Par√°metros de la \n",
    "\n",
    "# Create an instance of the neural network\n",
    "x_pinn = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in x_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 4 Y 5: DEFINICI√ìN DE LA FUNCI√ìN DE COSTO BASADA EN AUTOGRAD\n",
    "#==========================================================================\n",
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# derivatives of the ANN\n",
    "def PINNLoss(PINN, t_phys, wn, zeta, x0 = 1, v0 = 0, \n",
    "             w1 = 1, w2 = 1, w3 = 1):\n",
    "\n",
    "    t0 = torch.tensor(0., device=device, requires_grad=True).view(-1,1)\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    x_pinn_t = PINN(t_phys)\n",
    "    x_pinn_dt = grad(x_pinn_t, t_phys)\n",
    "    x_pinn_ddt = grad(x_pinn_dt, t_phys)\n",
    "    \n",
    "    f_ode = x_pinn_ddt + 2 * zeta * wn * x_pinn_dt + wn**2 * x_pinn_t\n",
    "    ODE_loss = w1 * MSE_func(f_ode, torch.zeros_like(f_ode)) \n",
    "    \n",
    "    g_ic = PINN(t0)\n",
    "    IC_loss = w2 * MSE_func(g_ic, torch.ones_like(g_ic)*x0)\n",
    "    \n",
    "    h_bc = grad(PINN(t0),t0)\n",
    "    BC_loss = w3 * MSE_func(h_bc, torch.zeros_like(h_bc)*v0)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss \n",
    "\n",
    "#==========================================================================\n",
    "# ETAPA 6: DEFINICI√ìN DEl OPTIMIZADOR\n",
    "#==========================================================================\n",
    "lr = 0.01\n",
    "optimizer = pinn_optimizer(x_pinn, lr)\n",
    "\n",
    "#==========================================================================\n",
    "# CICLO DE ENTRENAMIENTO\n",
    "#==========================================================================\n",
    "training_iter = 20000\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "loss_values_pinn = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINNLoss(x_pinn, t_train, wn, zeta)\n",
    "\n",
    "    # Append the current loss value to the list\n",
    "    loss_values_pinn.append(loss.item())\n",
    "\n",
    "    if i % 500 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "\n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaci√≥n\n",
    "\n",
    "El comportamiento del sistema depende de $\\zeta$:\n",
    "\n",
    "- **Subamortiguado** ($0 < \\zeta < 1$): el sistema oscila con frecuencia amortiguada  \n",
    "  \\begin{equation*}\n",
    "  \\omega_d = \\omega_n \\sqrt{1 - \\zeta^2}\n",
    "  \\end{equation*}\n",
    "\n",
    "  la soluci√≥n puede escribirse en t√©rminos de las condiciones iniciales $x(0)=x_0$ y $\\dot{x}(0)=v_0$:\n",
    "  \\begin{equation*}\n",
    "  x(t) = e^{-\\zeta \\omega_n t}\n",
    "  \\left[\n",
    "  x_0 \\cos(\\omega_d t) +\n",
    "  \\frac{v_0 + \\zeta \\omega_n x_0}{\\omega_d}\n",
    "  \\sin(\\omega_d t)\n",
    "  \\right]\n",
    "  \\end{equation*}\n",
    "\n",
    "- **Cr√≠ticamente amortiguado** ($\\zeta = 1$): no hay oscilaciones, el sistema retorna a equilibrio lo m√°s r√°pido posible sin sobrepasarlo:\n",
    "  \\begin{equation*}\n",
    "  x(t) = (x_0 + (v_0 + \\omega_n x_0)t)\\, e^{-\\omega_n t}\n",
    "  \\end{equation*}\n",
    "\n",
    "- **Sobreamortiguado** ($\\zeta > 1$): el sistema retorna al equilibrio sin oscilar, pero m√°s lentamente:\n",
    "  \\begin{equation*}\n",
    "  x(t) =\n",
    "  \\frac{v_0 - r_2 x_0}{r_1 - r_2} e^{r_1 t} +\n",
    "  \\frac{r_1 x_0 - v_0}{r_1 - r_2} e^{r_2 t}\n",
    "  \\end{equation*}\n",
    "\n",
    "  con:\n",
    "  \\begin{equation*}\n",
    "  r_{1,2} = -\\omega_n\\left( \\zeta \\pm \\sqrt{\\zeta^2 - 1} \\right)\n",
    "  \\end{equation*}\n",
    "\n",
    "\n",
    "Esta expresi√≥n servir√° como referencia para validar la soluci√≥n obtenida mediante la red neuronal informada por la f√≠sica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masa_resorte_general(t, x0, v0, omega_n, zeta):\n",
    "    \"\"\"\n",
    "    Soluci√≥n exacta x(t) del sistema masa-resorte-amortiguador:\n",
    "        x'' + 2*zeta*omega_n*x' + omega_n^2*x = 0\n",
    "    Incluye los tres reg√≠menes (sub, cr√≠tico y sobreamortiguado).\n",
    "    \"\"\"\n",
    "    t = np.array(t, dtype=float)\n",
    "\n",
    "    if 0 < zeta < 1:  # Subamortiguado\n",
    "        omega_d = omega_n * np.sqrt(1 - zeta**2)\n",
    "        x = np.exp(-zeta * omega_n * t) * (\n",
    "            x0 * np.cos(omega_d * t) +\n",
    "            (v0 + zeta * omega_n * x0) / omega_d * np.sin(omega_d * t)\n",
    "        )\n",
    "\n",
    "    elif np.isclose(zeta, 1.0):  # Cr√≠ticamente amortiguado\n",
    "        x = np.exp(-omega_n * t) * (\n",
    "            x0 * (1 + omega_n * t) + v0 * t\n",
    "        )\n",
    "\n",
    "    elif zeta > 1:  # Sobreamortiguado\n",
    "        r1 = -omega_n * (zeta - np.sqrt(zeta**2 - 1))\n",
    "        r2 = -omega_n * (zeta + np.sqrt(zeta**2 - 1))\n",
    "        x = (\n",
    "            ((v0 - r2 * x0) / (r1 - r2)) * np.exp(r1 * t) +\n",
    "            ((r1 * x0 - v0) / (r1 - r2)) * np.exp(r2 * t)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"zeta debe ser mayor que 0.\")\n",
    "\n",
    "    return torch.tensor(x, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "# -------------------------------------------\n",
    "# solucion exacta\n",
    "t_eval_np = t_eval.detach().cpu().numpy().ravel()\n",
    "x = masa_resorte_general(t_eval_np, x0, v0, wn, zeta)\n",
    "\n",
    "# predicci√≥n de la PINN\n",
    "x_pred_pinn = x_pinn(t_eval)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(x_pred_pinn, x)}')\n",
    "\n",
    "plot_comparison(t_eval, x, x_pred_pinn, loss_values_pinn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicios**:\n",
    "1. Ajusta los valores de los par√°metros del sistema din√°mico y analiza su impacto en la convergencia de la PINN.\n",
    "2. Ajusta los valores de los par√°metros `lambdas` en la funci√≥n de p√©rdida para ambas redes y analiza su impacto.\n",
    "2. Modifica la tasa de aprendizaje (`learning_rate`) del optimizador y el n√∫mero de iteraciones de entrenamiento, y eval√∫a el efecto en el desempe√±o.\n",
    "3. Cambia el n√∫mero de capas ocultas (`hidden_layers`), neuronas y funciones de activaci√≥n de la red neuronal, y observa el impacto en los resultados."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pinns-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
