{"cells":[{"cell_type":"markdown","metadata":{"id":"9MC6DyCDmZUq"},"source":["# Aplicaciones a Problemas Inversos\n","\n","**Por:** David Ortiz, Rodrigo Salas  \n","**Edici贸n:** David Ortiz, Tabita Catal谩n, Tom谩s Banduc  \n","\n","Referencia sobre estimaci贸n de par谩metros en problemas inversos:  \n","https://arxiv.org/abs/2308.00927\n","\n","## Introducci贸n\n","\n","En esta actividad se estudia el uso de PINNs para **problemas inversos**, extendiendo el enfoque aplicado previamente a problemas directos. El objetivo es estimar par谩metros desconocidos de un modelo f铆sico a partir de observaciones ruidosas. En particular, se abordar谩 la estimaci贸n del **coeficiente de difusi贸n** $\\kappa$ en un modelo de difusi贸n lineal.\n","\n","## Resumen de la actividad\n","\n","Se implementa una PINN para estimar el coeficiente de difusi贸n del modelo de calor unidimensional, tratando $\\kappa$ como un par谩metro entrenable junto con la soluci贸n del campo $u(t,x)$.\n","\n","## Objetivos\n","\n","Al finalizar esta actividad, ser谩s capaz de:\n","- Aplicar PINNs a la resoluci贸n de problemas inversos.  \n","- Entrenar una PINN para resolver una EDP y estimar simult谩neamente un par谩metro desconocido del modelo.\n","\n","## Descripci贸n matem谩tica del problema\n","\n","Consideramos nuevamente el modelo de difusi贸n unidimensional:\n","\n","$$\n","\\begin{aligned}\n","\\text{EDP:}\\quad &\n","\\frac{\\partial u}{\\partial t}\n","= \\kappa \\frac{\\partial^2 u}{\\partial x^2}\n","- e^{-t}\\big(\\sin(\\pi x) - \\pi^2 \\sin(\\pi x)\\big),\n","&& x \\in [-1,1],\\; t \\in [0,2], \\\\\n","\\text{Soluci贸n:}\\quad &\n","u(t,x) = e^{-t}\\sin(\\pi x).\n","\\end{aligned}\n","$$\n","\n","Aqu铆 $u(t,x)$ representa una cantidad f铆sica de inter茅s y $\\kappa$ es el coeficiente de difusi贸n, tratado como **par谩metro desconocido** durante el entrenamiento. En este caso, el valor verdadero es $\\kappa = 1$, que la PINN debe inferir a partir de los datos.\n","\n","A diferencia de los problemas directos, el entrenamiento se apoya en **observaciones ruidosas** $u_{\\text{data}}(t,x)$ de la soluci贸n anal铆tica, que aportan la informaci贸n necesaria para identificar el par谩metro.\n","\n","## Flujo de trabajo\n","\n","1. Generar observaciones ruidosas de la soluci贸n anal铆tica.  \n","2. Muestrear el dominio espaciotemporal para el entrenamiento.  \n","3. Entrenar la PINN para resolver la EDP y estimar simult谩neamente el par谩metro $\\kappa$.  \n","\n","## Recordemos el esquema b谩sico\n","<img src=\"../figures/pinns_new_scheme.png\" width=\"800\" height=\"400\">\n","\n","- (1) formular el modelo matem谩tico y su soluci贸n anal铆tica;  \n","- (2) definir el dominio espaciotemporal;  \n","- (3) implementar una ANN como aproximador de $u(t,x)$;  \n","- (4) emplear diferenciaci贸n autom谩tica;  \n","- (5) dise帽ar la *loss* con t茅rminos f铆sicos y de datos usando `autograd`;  \n","- (6) seleccionar un optimizador (Adam).  \n"]},{"cell_type":"markdown","metadata":{"id":"dx9qvQjdmZUr"},"source":["### Configuraci贸n Inicial\n","\n","Comenzamos importando m贸dulos y definiendo algunas funciones de utilidad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gufADy3jmZUr"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3G-YUetrKY4"},"outputs":[],"source":["# NumPy para operaciones num茅ricas\n","import numpy as np\n","# PyTorch para construir y entrenar redes neuronales\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","# Matplotlib para graficar\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","# Time para medir tiempo de entrenamiento\n","import time\n","# Warnings para ignorar mensajes de advertencia\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from matplotlib import animation, rc\n","from scipy.stats import qmc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9zNpH05mZUs"},"outputs":[],"source":["# Setup (device + plots)\n","def get_device() -> str:\n","    return \"cuda\" if torch.cuda.is_available() else \\\n","           \"mps\"  if torch.backends.mps.is_available() else \"cpu\"\n","\n","def set_mpl_style(gray: str = \"#5c5c5c\") -> None:\n","    mpl.rcParams.update({\n","        \"image.cmap\": \"viridis\",\n","        \"text.color\": gray, \"xtick.color\": gray, \"ytick.color\": gray,\n","        \"axes.labelcolor\": gray, \"axes.edgecolor\": gray,\n","        \"axes.spines.right\": False, \"axes.spines.top\": False,\n","        \"axes.formatter.use_mathtext\": True, \"axes.unicode_minus\": False,\n","        \"font.size\": 15, \"interactive\": False, \"font.family\": \"sans-serif\",\n","        \"legend.loc\": \"best\", \"text.usetex\": False, \"mathtext.fontset\": \"stix\",\n","    })\n","\n","device = get_device()\n","print(f\"Using {device} device\")\n","set_mpl_style()\n","\n","# Definir pi en torch\n","torch.pi = torch.acos(torch.zeros(1)).item() * 2\n","\n","def relative_l2_error(u_num: torch.Tensor, u_ref: torch.Tensor) -> torch.Tensor:\n","    return torch.norm(u_num - u_ref) / torch.norm(u_ref)\n","\n","# Autodiff helper\n","def grad(outputs: torch.Tensor, inputs: torch.Tensor) -> torch.Tensor:\n","    \"\"\"d(outputs)/d(inputs) with create_graph=True.\"\"\"\n","    return torch.autograd.grad(\n","        outputs, inputs,\n","        grad_outputs=torch.ones_like(outputs),\n","        create_graph=True\n","    )[0]\n","\n","def plot_comparison(u_true, u_pred, loss, k_evol):\n","    u_hat = u_pred.detach().cpu().numpy()\n","\n","    # --- Soluciones ---\n","    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","    for a, u, title in zip(\n","        ax,\n","        [u_true, u_hat],\n","        ['Analytic solution', 'PINN solution']\n","    ):\n","        im = a.imshow(u, extent=[-1, 1, 2, 0])\n","        a.set(title=title, xlabel=r'$x$', ylabel=r'$t$')\n","        fig.colorbar(im, ax=a, shrink=0.5)\n","\n","    fig.tight_layout()\n","    plt.show()\n","\n","    # --- Entrenamiento ---\n","    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n","\n","    ax[0].plot(k_evol, label='PINN estimate')\n","    ax[0].hlines(1, 0, len(k_evol), label='True value')\n","    ax[0].set(title=r'$\\kappa$ evolution', xlabel='Iteration')\n","    ax[0].legend(frameon=False)\n","\n","    ax[1].plot(loss)\n","    ax[1].set(title='Training Progress',\n","              xlabel='Iteration', ylabel='Loss',\n","              xscale='log', yscale='log')\n","    ax[1].grid(True)\n","\n","    fig.tight_layout()\n","    plt.show()\n","\n","def animate(x,t,U):\n","    # Primero, generar figura con subplot correspondiente\n","    fig, ax = plt.subplots(figsize = (10,6))\n","    plt.close()\n","    ax.set_title(r\"heat solution $e^t sin(\\pi x)$\")\n","    ax.set_xlim(x.min(), x.max())\n","    ax.set_ylim(np.floor(U.min()), np.ceil(U.max()))\n","    ax.set_xlabel(r\"$x$\")\n","    ax.set_ylabel(r\"$u(x, t)$\")\n","    #Inicializar etiqueta sin texto\n","    time_label = ax.text(1, 1, \"\", color = \"black\", fontsize = 12)\n","    #Inicializar gr谩fico sin datos\n","    line, = ax.plot([], [], color = \"black\", lw = 2)\n","\n","    # Definir funci贸n de inicializaci贸n\n","    def init():\n","        line.set_data([], [])\n","        time_label.set_text(\"\")\n","        return (line,)\n","\n","    # Animar funci贸n. Esta funci贸n se llama secuencialmente con FuncAnimation\n","    def animate(i):\n","        line.set_data(x, U[i])\n","        time_label.set_text(f\"t = {t[i]:.2f}\")\n","        return (line,)\n","\n","    return animation.FuncAnimation(fig, animate, init_func=init,\n","                                frames=len(t), interval=50, blit=True)\n"]},{"cell_type":"markdown","metadata":{"id":"ezTYQGE1mZUs"},"source":["## 1. Soluci贸n Anal铆tica\n","Una vez m谩s, definimos la soluci贸n anal铆tica $u(t,x) = e^{-t}\\sin(\\pi x)$, que ser谩 considerada la soluci贸n pura del problema para efectos de comparaci贸n. Para esta tarea, `analytic_diffusion` ser谩 usada para generar las observaciones de dicha funci贸n."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MfR-ZqemZUs"},"outputs":[],"source":["# N煤mero de muestras para espacio y tiempo.\n","dom_samples = 100\n","\n","# Funci贸n para soluci贸n anal铆tica\n","def analytic_diffusion(x,t):\n","    u = np.exp(-t)*np.sin(np.pi*x)\n","    return u\n","\n","# Dominio espacial\n","x = np.linspace(-1, 1, dom_samples)\n","# Dominio temporal\n","t = np.linspace(0, 2, dom_samples)\n","\n","# Mallado\n","X, T = np.meshgrid(x, t)\n","# Evaluar funci贸n en mallado\n","U = analytic_diffusion(X, T)\n","\n","fig = plt.figure(figsize=(12, 6))\n","ax = fig.add_subplot(111, projection='3d')\n","surf = ax.plot_surface(X, T, U, cmap='viridis', edgecolor='k')\n","\n","ax.set_xlabel('x')\n","ax.set_ylabel('t')\n","ax.set_zlabel('u(t, x)')\n","ax.set_title('3D Analytic Solution for Diffusion')\n","\n","# A帽adir la barra de color\n","fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceK-jhdW22I3"},"outputs":[],"source":["#Correr animaci贸n y hacer display\n","anim = animate(x,t,U)\n","rc(\"animation\", html=\"jshtml\")\n","anim"]},{"cell_type":"markdown","metadata":{"id":"YjQLki8YmZUs"},"source":["## 2. Muestreo del Dominio para entrenar la PINN\n","Para entrenar la PINN, aplicaremos la estrategia LHS (*Latin Hypercube Sampling*). Hay que recordar que LHS asegura que las muestras cubran uniformemente el espacio de entrada, previniendo la clusterizaci贸n en peque帽as 谩reas del dominio.\n","\n","Importamos `qmc.LatinHypercube` de `scipy.stats` y escalamos las muestras para adaptarlas al borde del dominio. Adicionalmente, convertimos el dominio temporal y las observaciones a `torch.tensors` para compatibilizarlas con la PINN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrUou0elmZUt"},"outputs":[],"source":["from scipy.stats import qmc\n","# Muestreo con LHS\n","sampler = qmc.LatinHypercube(d=2)\n","sample = sampler.random(n=100)\n","\n","# L铆mites del dominio\n","l_bounds = [-1, 0]\n","u_bounds = [ 1, 2]\n","domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n","\n","# Tensores de torch\n","x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n","t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n","\n","fig, ax = plt.subplots(figsize=(6, 6))\n","ax.scatter(domain_xt[:, 0],domain_xt[:, 1], label = 'PDE collocation points')\n","ax.set_title('Collocation points')\n","ax.set_xlabel(r'$x$')\n","ax.set_ylabel(r'$t$')\n","ax.legend(loc='lower left')\n","plt.gca().invert_yaxis()\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"eBPdBnaImZUt"},"source":["Adem谩s, evaluamos `analytic_diffusion` en los puntos de colocaci贸n. La inclusi贸n de ruido se sigue del supuesto que las observaciones vienen dadas de la superposici贸n entre la soluci贸n anal铆tica y alguna variable aleatoria (t铆picamente gaussiana) generada por defecto durante la medici贸n, i.e., $u_{data}(t,x)=u(t,x)+\\varepsilon$, con $\\varepsilon\\sim \\mathcal{N}(0,\\sigma)$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WorttQggmZUt"},"outputs":[],"source":["# Evaluar puntos en funci贸n anal铆tica\n","x_np = x_ten.detach().numpy()\n","t_np = t_ten.detach().numpy()\n","u_true = analytic_diffusion(x_np,t_np).reshape(1, -1)\n","u_observ = u_true + np.random.normal(0,0.01,len(x_np))\n","# Convertir observaciones a torch\n","u_observ_t = torch.tensor(u_observ, requires_grad = True).float().reshape(-1,1)"]},{"cell_type":"markdown","metadata":{"id":"2XCH8uHUmZUt"},"source":["## 3. Soluci贸n del problema inverso con PINNs\n","\n","En este apartado se aborda la estimaci贸n del par谩metro de difusi贸n $\\kappa$ de forma simult谩nea al entrenamiento de la PINN. Para ello, la red neuronal aproxima directamente la soluci贸n de la EDP y se introduce $\\kappa$ como un **par谩metro entrenable adicional**:\n","\n","$$\n","u_{\\text{PINN}}(t,x;\\Theta,\\kappa) \\approx u(t,x),\n","$$\n","\n","donde $\\Theta$ denota los par谩metros de la red y $\\kappa$ el par谩metro f铆sico desconocido. La implementaci贸n se realiza en `PyTorch`, manteniendo la misma arquitectura utilizada en los problemas directos.\n","\n","> ** Nota**  \n","> En problemas inversos, los par谩metros f铆sicos desconocidos (como $\\kappa$) se tratan como variables entrenables y deben incorporarse expl铆citamente al proceso de optimizaci贸n.\n","\n","### Funci贸n de p茅rdida informada por la f铆sica\n","\n","Para entrenar la PINN se define el residuo de la ecuaci贸n de calor, reemplazando la soluci贸n exacta por la salida de la red:\n","\n","$$\n","f_{\\text{pde}}(t,x;u_{\\text{PINN}}) =\n","\\frac{\\partial u_{\\text{PINN}}}{\\partial t}\n","- \\kappa \\frac{\\partial^2 u_{\\text{PINN}}}{\\partial x^2}\n","+ e^{-t}\\big(\\sin(\\pi x) - \\pi^2 \\sin(\\pi x)\\big).\n","$$\n","\n","> ** Nota**  \n","> Al tratarse de un problema inverso, no se imponen condiciones iniciales ni de borde; la informaci贸n necesaria proviene de los datos observados.\n","\n","La funci贸n de p茅rdida se construye mediante el error cuadr谩tico medio (MSE), combinando el residuo f铆sico y el ajuste a los datos:\n","\n","$$\n","\\begin{aligned}\n","\\mathcal{L}(\\Theta,\\kappa) =\\;&\n","\\frac{\\lambda_1}{N}\\sum_i f_{\\text{pde}}(t_i,x_i;u_{\\text{PINN}})^2 \\\\\n","&+ \\frac{\\lambda_2}{N}\\sum_i\n","\\big(u_{\\text{PINN}}(t_i,x_i;\\Theta,\\kappa) - u_{\\text{data}}(t_i,x_i)\\big)^2,\n","\\end{aligned}\n","$$\n","\n","donde $\\lambda_{1,2} \\in \\mathbb{R}^+$ son coeficientes de ponderaci贸n y $N$ es el n煤mero de muestras.\n","\n","> ** Nota**  \n","> Un problema inverso se aborda mediante un esquema *data-driven*, ya que la estimaci贸n del par谩metro requiere informaci贸n proveniente de observaciones.\n","\n","El entrenamiento se formula como el problema de optimizaci贸n\n","\n","$$\n","\\min_{\\Theta,\\kappa} \\; \\mathcal{L}(\\Theta,\\kappa) \\rightarrow 0.\n","$$\n","\n","### Definici贸n del optimizador\n","\n","Una vez definida la funci贸n de p茅rdida, se selecciona un optimizador para ajustar **tanto** los par谩metros de la red $\\Theta = \\{W_i,b_i\\}$ **como** el par谩metro f铆sico $\\kappa$. El objetivo es minimizar la funci贸n de p茅rdida informada por la f铆sica y los datos mediante un esquema iterativo:\n","\n","$$\n","\\begin{aligned}\n","\\Theta^{k+1} &= \\Theta^{k} - \\alpha \\nabla_{\\Theta}\\mathcal{L}(\\Theta^{k},\\kappa^{k}), \\\\\n","\\kappa^{k+1} &= \\kappa^{k} - \\alpha \\nabla_{\\kappa}\\mathcal{L}(\\Theta^{k},\\kappa^{k}).\n","\\end{aligned}\n","$$\n","\n","En la pr谩ctica, ambos conjuntos de par谩metros se incluyen conjuntamente en el optimizador (por ejemplo, **Adam**), permitiendo que la PINN aprenda simult谩neamente la soluci贸n de la EDP y el valor del coeficiente de difusi贸n.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oK142NkEmZUt"},"outputs":[],"source":["# Definir clase de red neuronal con capas y neuronas especificadas por usuario\n","class NeuralNetwork(nn.Module):\n","\n","    def __init__(self, hlayers):\n","        super(NeuralNetwork, self).__init__()\n","\n","        layers = []\n","        for i in range(len(hlayers[:-2])):\n","            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n","            layers.append(nn.Tanh())\n","        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n","\n","        self.layers = nn.Sequential(*layers)\n","        self.init_params()\n","\n","    def init_params(self):\n","        \"\"\"Inicializaci贸n de par谩metros Xavier Glorot\n","        \"\"\"\n","        def init_normal(m):\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_normal_(m.weight) # Xavier\n","        self.apply(init_normal)\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","#===============================================================================\n","# ETAPA 3: CREACIN DE LA RED NEURONAL SURROGANTE\n","#===============================================================================\n","torch.manual_seed(123)\n","\n","# hiper-par谩metros de la red\n","hidden_layers = [2, 10, 10, 10, 1]\n","\n","# Crear instancia de la NN\n","u_pinn = NeuralNetwork(hidden_layers)\n","nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n","print(f'Number of trainable parameters: {nparams}')\n","\n","#==========================================================================\n","# ETAPA 6: DEFINICIN DEl OPTIMIZADOR\n","#==========================================================================\n","learning_rate = 0.001\n","\n","# Tratar k como par谩metro entrenable\n","kappa = torch.nn.Parameter(torch.ones(1, requires_grad=True)*2)\n","kappas = []\n","\n","# Definir optimizador y agregar k\n","optimizer = optim.Adam(list(u_pinn.parameters())+[kappa], lr=0.001,\n","                       betas= (0.99,0.999), eps = 1e-8)\n"]},{"cell_type":"markdown","metadata":{"id":"VQ6l285b22I-"},"source":["A continuaci贸n se presenta el c贸digo completo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3l-0cre22I-"},"outputs":[],"source":["#===============================================================================\n","# ETAPA 1: DEFINICIN DE LOS PARMETROS (MODELO FSICO)\n","#===============================================================================\n","from scipy.stats import qmc\n","# Muestreo con LHS\n","sampler = qmc.LatinHypercube(d=2)\n","sample = sampler.random(n=100)\n","\n","# L铆mites del dominio\n","l_bounds = [-1, 0]\n","u_bounds = [ 1, 2]\n","domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n","\n","# Funci贸n para soluci贸n anal铆tica\n","def analytic_diffusion(x,t):\n","    u = np.exp(-t)*np.sin(np.pi*x)\n","    return u\n","\n","# Evaluar puntos en funci贸n anal铆tica\n","x_np = domain_xt[:, 0]\n","t_np = domain_xt[:, 1]\n","u_true = analytic_diffusion(x_np,t_np).reshape(1, -1)\n","u_observ = u_true + np.random.normal(0,0.01,len(x_np))\n","# Convertir observaciones a torch\n","u_observ_t = torch.tensor(u_observ, requires_grad = True).float().reshape(-1,1)\n","\n","#===============================================================================\n","# ETAPA 2: DEFINICIN DEL DOMINIO\n","#===============================================================================\n","# Tensores de torch\n","x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n","t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n","\n","#===============================================================================\n","# ETAPA 3: CREACIN DE LA RED NEURONAL SURROGANTE\n","#===============================================================================\n","torch.manual_seed(123)\n","\n","# hiper-par谩metros de la red\n","hidden_layers = [2, 10, 10, 10, 1]\n","\n","# Crear instancia de la NN\n","u_pinn = NeuralNetwork(hidden_layers)\n","nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n","print(f'Number of trainable parameters: {nparams}')\n","\n","#==========================================================================\n","# ETAPA 4 Y 5: DEFINICIN DE LA FUNCIN DE COSTO BASADA EN AUTOGRAD\n","#==========================================================================\n","# Error cuadr谩tico medio (Mean Squared Error - MSE)\n","MSE_func = nn.MSELoss()\n","\n","def PINN_diffusion_Loss(forward_pass, x_ten, t_ten, kappa,\n","             lambda1 = 1, lambda2 = 1):\n","\n","    # define domain\n","    domain = torch.cat([t_ten, x_ten], dim = 1)\n","\n","    # define the ANN output\n","    u = forward_pass(domain)\n","\n","    # TODO: compute the first derivative in time, and first and second derivatives in space\n","    u_t = ...\n","    u_x = ...\n","    u_xx = ...\n","\n","    # TODO: define the PDE loss\n","    f_pde = ...\n","    PDE_loss = lambda1 * ...\n","\n","    # TODO define the data loss\n","    data_loss = lambda2 * ... # you should use u_observ_t\n","\n","    return PDE_loss + data_loss\n","\n","#==========================================================================\n","# ETAPA 6: DEFINICIN DEl OPTIMIZADOR\n","#==========================================================================\n","learning_rate = 0.001\n","\n","# Tratar k como par谩metro entrenable\n","kappa = torch.nn.Parameter(torch.ones(1, requires_grad=True)*2)\n","kappas = []\n","\n","# Definir optimizador y agregar k\n","optimizer = optim.Adam(list(u_pinn.parameters())+[kappa], lr=0.001,\n","                       betas= (0.99,0.999), eps = 1e-8)\n","\n","\n","#==========================================================================\n","# CICLO DE ENTRENAMIENTO\n","#==========================================================================\n","\n","training_iter = 25000\n","\n","# Inicializar lista para guardar valores de p茅rdida\n","loss_values = []\n","\n","# Empezar timer\n","start_time = time.time()\n","\n","# Entrenar red neuronal\n","for i in range(training_iter):\n","\n","    optimizer.zero_grad()   # Reinicializar gradientes para iteraci贸n de entrenamiento\n","\n","    # ingresar x, predecir con PINN y obtener p茅rdida\n","    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten, kappa)\n","\n","    # Agregar actual valor de p茅rdida a la lista y agregar valor de kappa actual\n","    loss_values.append(loss.item())\n","    kappas.append(kappa.item())\n","\n","    if i % 1000 == 0:  # Imprimir p茅rdida cada 1000 iteraciones\n","        print(f\"Iteration {i}: Loss {loss.item()}\")\n","\n","    loss.backward() # Paso de retropropagaci贸n\n","    optimizer.step() # Actualizar pesos de la red con optimizador\n","\n","# Detener timer y obtener tiempo transcurrido\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Training time: {elapsed_time} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRc2wEt0mZUu"},"outputs":[],"source":["dom_samples = 100\n","\n","# Dominio espacial\n","x = np.linspace(-1, 1, dom_samples)\n","# Dominio temporal\n","t = np.linspace(0, 2, dom_samples)\n","\n","# Mallado\n","X, T = np.meshgrid(x, t)\n","\n","# Evaluar funci贸n en mallado\n","U = analytic_diffusion(X, T)\n","\n","X_ten = torch.tensor(X).float().reshape(-1, 1)\n","T_ten = torch.tensor(T).float().reshape(-1, 1)\n","domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n","U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n","\n","U_true = torch.tensor(U).float()\n","print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n","\n","plot_comparison(U, U_pred, loss_values, kappas)"]},{"cell_type":"markdown","metadata":{"id":"Y_6L95FLmZUu"},"source":["**Ejercicios**:\n","1. Disminuya y aumente el valor inicial del par谩metro $\\kappa$. Considere que en varias aplicaciones solo se conoce un intervalo de b煤squeda para los par谩metros desconocidos.\n","2. Eval煤e c贸mo var铆a la soluci贸n de la PINN aumentando y disminuyendo los pesos `lambdas`.\n","3. Eval煤e c贸mo var铆a la soluci贸n de la PINN aumentando y disminuyendo la tasa de aprendizaje y el n煤mero de iteraciones de entrenamiento.\n","4. Cambie el n煤mero de capas ocultas, neuronas y funciones de activaci贸n del modelo de NN.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"pinns-tutorial","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.11"}},"nbformat":4,"nbformat_minor":0}